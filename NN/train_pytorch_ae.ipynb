{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ea1afdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a20f50b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aai\\AppData\\Local\\Temp\\ipykernel_23332\\3161401370.py:11: DtypeWarning: Columns (0,1,2,3,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(lut_path, names=headers.split(\",\"), header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Cm          Ch Bm Bh          T       sR       sG       sB\n",
      "1  0.001  0.00760226  0  0       0.01  250.349  201.031   164.79\n",
      "2  0.001  0.00760226  0  0  0.0271429  251.037  202.084  167.913\n",
      "3  0.001  0.00760226  0  0  0.0785714  251.832  203.648   172.33\n",
      "4  0.001  0.00760226  0  0  0.0614286  251.309  202.753  170.808\n",
      "5  0.001  0.00760226  0  0  0.0957143  252.102  204.416  174.004\n",
      "749373\n",
      "      Cm        Ch   Bm   Bh         T     sR     sG     sB\n",
      "1  0.001  0.007602  0.0  0.0  0.010000  250.0  201.0  165.0\n",
      "2  0.001  0.007602  0.0  0.0  0.027143  251.0  202.0  168.0\n",
      "3  0.001  0.007602  0.0  0.0  0.078571  252.0  204.0  172.0\n",
      "4  0.001  0.007602  0.0  0.0  0.061429  251.0  203.0  171.0\n",
      "5  0.001  0.007602  0.0  0.0  0.095714  252.0  204.0  174.0\n",
      "length of df 876726\n",
      "bef norm x_train[0] [99.108 48.952 39.707]\n",
      "aft norm x_train[0] [0.38865882 0.19196862 0.15571372]\n",
      "length of x_train 701379\n",
      "length of x_test 175345\n",
      "length of y_train 701379\n",
      "length of y_test 175345\n",
      "length of df 876726\n",
      "Cm = [np.float64(0.001), np.float64(0.00443404), np.float64(0.0119417), np.float64(0.025117), np.float64(0.0455539), np.float64(0.0748466), np.float64(0.114589), np.float64(0.166375), np.float64(0.231799), np.float64(0.312454), np.float64(0.409936), np.float64(0.525837), np.float64(0.661752), np.float64(0.819275), np.float64(1.0)]\n",
      "Ch = [np.float64(0.001), np.float64(0.0031313), np.float64(0.00760226), np.float64(0.0157055), np.float64(0.029019), np.float64(0.0494063), np.float64(0.0790165), np.float64(0.120284), np.float64(0.175928), np.float64(0.248956), np.float64(0.342656), np.float64(0.460606), np.float64(0.606668), np.float64(0.784988), np.float64(1.0)]\n",
      "Bm = [np.float64(0.0), np.float64(0.00510204), np.float64(0.0204082), np.float64(0.0459184), np.float64(0.0816327), np.float64(0.127551), np.float64(0.183673), np.float64(0.25), np.float64(0.326531), np.float64(0.413265), np.float64(0.510204), np.float64(0.617347), np.float64(0.734694), np.float64(0.862245), np.float64(1.0)]\n",
      "Bh = [np.float64(0.0), np.float64(0.0714286), np.float64(0.142857), np.float64(0.214286), np.float64(0.285714), np.float64(0.357143), np.float64(0.428571), np.float64(0.5), np.float64(0.571429), np.float64(0.642857), np.float64(0.714286), np.float64(0.785714), np.float64(0.857143), np.float64(0.928571), np.float64(1.0)]\n",
      "T = [np.float64(0.01), np.float64(0.0271429), np.float64(0.0442857), np.float64(0.0614286), np.float64(0.0785714), np.float64(0.0957143), np.float64(0.112857), np.float64(0.13), np.float64(0.147143), np.float64(0.164286), np.float64(0.181429), np.float64(0.198571), np.float64(0.215714), np.float64(0.232857), np.float64(0.25)]\n",
      "upper bounds = [np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(1.0), np.float64(0.25)]\n",
      "lower bounds = [np.float64(0.001), np.float64(0.001), np.float64(0.0), np.float64(0.0), np.float64(0.01)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aai\\AppData\\Local\\Temp\\ipykernel_23332\\3161401370.py:109: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[['sR', 'sG', 'sB']] = df[['sR', 'sG', 'sB']].applymap(np.round)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of repeated RGB values 780784\n"
     ]
    }
   ],
   "source": [
    "upper_bounds = [0.99, 0.99, 0.99, 0.99, 0.25]\n",
    "lower_bounds = [0.01, 0.001, 0.01, 0.6, 0.05]\n",
    "\n",
    "# Pair up the corresponding bounds and calculate their average\n",
    "averages = [(u + l) / 2 for u, l in zip(upper_bounds, lower_bounds)]\n",
    "avg_Cm, avg_Ch, avg_Bm, avg_Bh, avg_T = averages\n",
    "\n",
    "headers = \"Cm,Ch,Bm,Bh,T,sR,sG,sB\"\n",
    "lut_path = r\"D:\\Github\\Deep-Albedo\\cpp_monte_carlo\\lut_rgb.csv\"\n",
    "\n",
    "df = pd.read_csv(lut_path, names=headers.split(\",\"), header=None)\n",
    "#remove row 0\n",
    "df = df.iloc[1:]\n",
    "print(df.head())\n",
    "\n",
    "#print length of df\n",
    "print(len(df))\n",
    "# convert all columns to float\n",
    "for column in df.columns:\n",
    "    df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "#convert sR, sG, sB to int\n",
    "rounded_df = df.round({'sR': 0, 'sG': 0, 'sB': 0})\n",
    "# Step 1: Find duplicate RGB values\n",
    "duplicates = rounded_df[rounded_df.duplicated(subset=['sR', 'sG', 'sB'], keep=False)].copy() # added .copy()\n",
    "#print all duplicates\n",
    "print(duplicates.head())\n",
    "\n",
    "# Step 2: Calculate the 'likelihood' score for each row\n",
    "duplicates['likelihood'] = (abs(duplicates['Cm'] - avg_Cm) +\n",
    "                            abs(duplicates['Ch'] - avg_Ch) +\n",
    "                            abs(duplicates['Bm'] - avg_Bm) +\n",
    "                            abs(duplicates['Bh'] - avg_Bh) +\n",
    "                            abs(duplicates['T'] - avg_T))\n",
    "\n",
    "# Step 3: Sort by RGB values and likelihood, keeping the row with the lowest likelihood for each RGB group\n",
    "most_likely_duplicates = duplicates.sort_values(['sR', 'sG', 'sB', 'likelihood']).drop_duplicates(subset=['sR', 'sG', 'sB'])\n",
    "\n",
    "# Now, most_likely_duplicates should contain your desired rows\n",
    "\n",
    "# First, remove all duplicates from the original dataframe\n",
    "df_no_duplicates = df.drop_duplicates(subset=['sR', 'sG', 'sB'], keep=False)\n",
    "\n",
    "# Concatenate df_no_duplicates with most_likely_duplicates to get the final dataframe\n",
    "df = pd.concat([df_no_duplicates, most_likely_duplicates])\n",
    "\n",
    "# If you want to sort it based on index\n",
    "df.sort_index(inplace=True)\n",
    "df.head()\n",
    "#remove duplicates\n",
    "\n",
    "x = df[['sR', 'sG', 'sB']].to_numpy(dtype='float32')\n",
    "y = df[['Cm', 'Ch', 'Bm', 'Bh', 'T']].to_numpy(dtype='float32')\n",
    "#create new csv with headers\n",
    "# df.to_csv(r'LUTs\\large_no_duplicates.csv', index=False, header=True)\n",
    "\n",
    "\n",
    "#train nn on x,y\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "#remove any header values\n",
    "x_train = x_train[1:]\n",
    "x_test = x_test[1:]\n",
    "y_train = y_train[1:]\n",
    "y_test = y_test[1:]\n",
    "\n",
    "#numpy arrays\n",
    "x_train = np.asarray(x_train).reshape(-1,3).astype('float32')\n",
    "x_test = np.asarray(x_test).reshape(-1,3).astype('float32')\n",
    "print(f\"length of df {len(df)}\")\n",
    "print(f\"bef norm x_train[0] {x_train[0]}\")\n",
    "\n",
    "#normalize\n",
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0\n",
    "print(f\"aft norm x_train[0] {x_train[0]}\")\n",
    "\n",
    "print(f\"length of x_train {len(x_train)}\")\n",
    "print(f\"length of x_test {len(x_test)}\")\n",
    "print(f\"length of y_train {len(y_train)}\")\n",
    "print(f\"length of y_test {len(y_test)}\")\n",
    "df.head()\n",
    "print(f\"length of df {len(df)}\")\n",
    "#print random 3 rows\n",
    "#print unique values of Cm,Ch,Bm,Bh,T\n",
    "# print(f\"unique Cm {df['Cm'].unique()}\")\n",
    "# print(f\"unique Ch {df['Ch'].unique()}\")\n",
    "# print(f\"unique Bm {df['Bm'].unique()}\")\n",
    "# print(f\"unique Bh {df['Bh'].unique()}\")\n",
    "# print(f\"unique T {df['T'].unique()}\")\n",
    "#as sorted lists\n",
    "C_m = sorted(df['Cm'].unique())\n",
    "C_h = sorted(df['Ch'].unique())\n",
    "B_m = sorted(df['Bm'].unique())\n",
    "B_h = sorted(df['Bh'].unique())\n",
    "T = sorted(df['T'].unique())\n",
    "print(f\"Cm = {C_m}\")\n",
    "print(f\"Ch = {C_h}\")\n",
    "print(f\"Bm = {B_m}\")\n",
    "print(f\"Bh = {B_h}\")\n",
    "print(f\"T = {T}\")\n",
    "#min max for each\n",
    "min_vals = [min(C_m), min(C_h), min(B_m), min(B_h), min(T)]\n",
    "max_vals = [max(C_m), max(C_h), max(B_m), max(B_h), max(T)]\n",
    "print(f\"upper bounds = {max_vals}\")\n",
    "print(f\"lower bounds = {min_vals}\")\n",
    "#integer arrays for sR,sG,sB 0 to 255\n",
    "# Assuming df is your DataFrame and it has columns 'sR', 'sG', 'sB'\n",
    "df[['sR', 'sG', 'sB']] = df[['sR', 'sG', 'sB']].astype(float)\n",
    "df[['sR', 'sG', 'sB']] = df[['sR', 'sG', 'sB']].applymap(np.round)\n",
    "# Add a 'count' column that counts the number of identical RGB values\n",
    "df['count'] = df.groupby(['sR', 'sG', 'sB'])['sR'].transform('count')\n",
    "print(f\"number of repeated RGB values {len(df[df['count'] > 1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b922472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Reproducibility (np.random.seed(7) equivalent)\n",
    "# -----------------------------\n",
    "np.random.seed(7)\n",
    "torch.manual_seed(7)\n",
    "torch.cuda.manual_seed_all(7)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -----------------------------\n",
    "# Hyperparameters (from your code)\n",
    "# -----------------------------\n",
    "BATCH_SIZE = 4096 * 16\n",
    "NUM_NEURONS = 75\n",
    "NUM_LAYERS = 2\n",
    "NUM_EPOCHS = 200\n",
    "LR = 1e-4\n",
    "MLR = 1e-6  # unused in your compile; keeping for parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "848fb777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class AEDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        \"\"\"\n",
    "        x: [N,3]  (albedo / RGB in your naming)\n",
    "        y: [N,5]  (parameters / latent in your naming)\n",
    "        \"\"\"\n",
    "        self.x = torch.as_tensor(x, dtype=torch.float32)\n",
    "        self.y = torch.as_tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_i = self.x[idx]   # [3]\n",
    "        y_i = self.y[idx]   # [5]\n",
    "\n",
    "        # inputs (match Keras x=[x_train, y_train, x_train])\n",
    "        enc_in = x_i\n",
    "        dec_in = y_i\n",
    "        end_in = x_i\n",
    "\n",
    "        # targets (match Keras y=[y_train, x_train, x_train])\n",
    "        enc_true = y_i\n",
    "        dec_true = x_i\n",
    "        end_true = x_i\n",
    "\n",
    "        return enc_in, dec_in, end_in, enc_true, dec_true, end_true\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    AEDataset(x_train, y_train),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    AEDataset(x_test, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07f934a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import json\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# DEVICE\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# MODEL COMPONENTS\n",
    "# -----------------------------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim=3, hidden_dim=NUM_NEURONS, num_layers=NUM_LAYERS, out_dim=5):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Linear(in_dim if i == 0 else hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.out = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_dim=5, hidden_dim=NUM_NEURONS, num_layers=NUM_LAYERS, out_dim=3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Linear(in_dim if i == 0 else hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.out = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, encoder: nn.Module, decoder: nn.Module):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, encoder_in, decoder_in, end_to_end_in):\n",
    "        enc_out = self.encoder(encoder_in)\n",
    "        dec_out = self.decoder(decoder_in)\n",
    "        end_out = self.decoder(self.encoder(end_to_end_in))\n",
    "        return enc_out, dec_out, end_out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Loss functions (same as before)\n",
    "# -----------------------------\n",
    "def albedo_loss(y_true, y_pred):\n",
    "    return torch.sum(torch.abs(y_pred - y_true), dim=-1)\n",
    "\n",
    "def parameter_loss(y_true, y_pred):\n",
    "    return torch.sqrt(torch.sum((y_pred - y_true) ** 2, dim=-1) + 1e-12)\n",
    "\n",
    "def end_to_end_loss(y_true, y_pred):\n",
    "    return torch.sum(torch.abs(y_pred - y_true), dim=-1)\n",
    "\n",
    "def reduce_loss(loss_per_sample, reduction=\"mean\"):\n",
    "    if reduction == \"mean\":\n",
    "        return loss_per_sample.mean()\n",
    "    elif reduction == \"sum\":\n",
    "        return loss_per_sample.sum()\n",
    "    else:\n",
    "        raise ValueError(\"reduction must be 'mean' or 'sum'\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Run folder creation (date/time)\n",
    "# -----------------------------\n",
    "def create_run_folder(base_dir=\"checkpoints\"):\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    run_dir = os.path.join(base_dir, now)\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    return run_dir\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation loop (val)\n",
    "# -----------------------------\n",
    "def evaluate(model, dataloader, loss_weights=(0.3, 0.1, 0.6)):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss_sum = 0.0\n",
    "    loss1_sum = 0.0\n",
    "    loss2_sum = 0.0\n",
    "    loss3_sum = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            enc_in, dec_in, end_in, enc_true, dec_true, end_true = batch\n",
    "\n",
    "            enc_in   = enc_in.to(device)\n",
    "            dec_in   = dec_in.to(device)\n",
    "            end_in   = end_in.to(device)\n",
    "            enc_true = enc_true.to(device)\n",
    "            dec_true = dec_true.to(device)\n",
    "            end_true = end_true.to(device)\n",
    "\n",
    "            enc_pred, dec_pred, end_pred = model(enc_in, dec_in, end_in)\n",
    "\n",
    "            loss1 = reduce_loss(parameter_loss(enc_true, enc_pred), \"mean\")\n",
    "            loss2 = reduce_loss(albedo_loss(dec_true, dec_pred), \"mean\")\n",
    "            loss3 = reduce_loss(end_to_end_loss(end_true, end_pred), \"mean\")\n",
    "\n",
    "            total_loss = loss_weights[0]*loss1 + loss_weights[1]*loss2 + loss_weights[2]*loss3\n",
    "\n",
    "            total_loss_sum += total_loss.item()\n",
    "            loss1_sum += loss1.item()\n",
    "            loss2_sum += loss2.item()\n",
    "            loss3_sum += loss3.item()\n",
    "            n_batches += 1\n",
    "\n",
    "    if n_batches == 0:\n",
    "        return {\"total\": 0.0, \"loss1\": 0.0, \"loss2\": 0.0, \"loss3\": 0.0}\n",
    "\n",
    "    return {\n",
    "        \"total\": total_loss_sum / n_batches,\n",
    "        \"loss1\": loss1_sum / n_batches,\n",
    "        \"loss2\": loss2_sum / n_batches,\n",
    "        \"loss3\": loss3_sum / n_batches,\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training loop (train + val)\n",
    "# -----------------------------\n",
    "def train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    num_epochs=200,\n",
    "    loss_weights=(0.3, 0.1, 0.6),\n",
    "    base_ckpt_dir=\"checkpoints\",\n",
    "    checkpoint_period=200,\n",
    "    print_period=25,\n",
    "    save_json_each_epoch=True,\n",
    "):\n",
    "    run_dir = create_run_folder(base_ckpt_dir)\n",
    "    best_ckpt_path = os.path.join(run_dir, \"best.pt\")\n",
    "    last_ckpt_path = os.path.join(run_dir, \"last.pt\")\n",
    "    json_path = os.path.join(run_dir, \"history.json\")\n",
    "\n",
    "    print(\"Using device:\", device)\n",
    "    print(\"Run folder:\", run_dir)\n",
    "\n",
    "    best_train_loss = float(\"inf\")\n",
    "\n",
    "    history = {\n",
    "        \"config\": {\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"loss_weights\": loss_weights,\n",
    "            \"optimizer\": optimizer.__class__.__name__,\n",
    "            \"scheduler\": scheduler.__class__.__name__,\n",
    "            \"device\": str(device),\n",
    "        },\n",
    "        \"epochs\": []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        total_loss_sum = 0.0\n",
    "        loss1_sum = 0.0\n",
    "        loss2_sum = 0.0\n",
    "        loss3_sum = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            enc_in, dec_in, end_in, enc_true, dec_true, end_true = batch\n",
    "\n",
    "            enc_in   = enc_in.to(device)\n",
    "            dec_in   = dec_in.to(device)\n",
    "            end_in   = end_in.to(device)\n",
    "            enc_true = enc_true.to(device)\n",
    "            dec_true = dec_true.to(device)\n",
    "            end_true = end_true.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            enc_pred, dec_pred, end_pred = model(enc_in, dec_in, end_in)\n",
    "\n",
    "            loss1 = reduce_loss(parameter_loss(enc_true, enc_pred), \"mean\")\n",
    "            loss2 = reduce_loss(albedo_loss(dec_true, dec_pred), \"mean\")\n",
    "            loss3 = reduce_loss(end_to_end_loss(end_true, end_pred), \"mean\")\n",
    "\n",
    "            total_loss = loss_weights[0]*loss1 + loss_weights[1]*loss2 + loss_weights[2]*loss3\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss_sum += total_loss.item()\n",
    "            loss1_sum += loss1.item()\n",
    "            loss2_sum += loss2.item()\n",
    "            loss3_sum += loss3.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        train_stats = {\n",
    "            \"total\": total_loss_sum / max(n_batches, 1),\n",
    "            \"loss1\": loss1_sum / max(n_batches, 1),\n",
    "            \"loss2\": loss2_sum / max(n_batches, 1),\n",
    "            \"loss3\": loss3_sum / max(n_batches, 1),\n",
    "        }\n",
    "\n",
    "        val_stats = evaluate(model, val_loader, loss_weights=loss_weights)\n",
    "\n",
    "        # Keras ReduceLROnPlateau monitors train loss in your code\n",
    "        scheduler.step(train_stats[\"total\"])\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        # Logging record for this epoch\n",
    "        epoch_record = {\n",
    "            \"epoch\": epoch,\n",
    "            \"lr\": current_lr,\n",
    "            \"train\": train_stats,\n",
    "            \"val\": val_stats,\n",
    "        }\n",
    "        history[\"epochs\"].append(epoch_record)\n",
    "\n",
    "        # Save JSON file continuously (optional)\n",
    "        if save_json_each_epoch:\n",
    "            with open(json_path, \"w\") as f:\n",
    "                json.dump(history, f, indent=4)\n",
    "\n",
    "        # Print callback every N epochs\n",
    "        if epoch % print_period == 0:\n",
    "            print(\n",
    "                f\"epoch: {epoch} | \"\n",
    "                f\"train_total={train_stats['total']:.6f} \"\n",
    "                f\"(L1={train_stats['loss2']:.6f}, L2={train_stats['loss1']:.6f}, end={train_stats['loss3']:.6f}) | \"\n",
    "                f\"val_total={val_stats['total']:.6f} \"\n",
    "                f\"(L1={val_stats['loss2']:.6f}, L2={val_stats['loss1']:.6f}, end={val_stats['loss3']:.6f}) | \"\n",
    "                f\"lr={current_lr:.2e}\"\n",
    "            )\n",
    "\n",
    "        # Save BEST model (like ModelCheckpoint(save_best_only=True))\n",
    "        if train_stats[\"total\"] < best_train_loss:\n",
    "            best_train_loss = train_stats[\"total\"]\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"best_train_loss\": best_train_loss,\n",
    "                    \"history_path\": json_path,\n",
    "                },\n",
    "                best_ckpt_path,\n",
    "            )\n",
    "\n",
    "        # Save periodic checkpoint (every checkpoint_period epochs)\n",
    "        if (epoch + 1) % checkpoint_period == 0:\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"train_total_loss\": train_stats[\"total\"],\n",
    "                    \"val_total_loss\": val_stats[\"total\"],\n",
    "                    \"history_path\": json_path,\n",
    "                },\n",
    "                last_ckpt_path,\n",
    "            )\n",
    "\n",
    "    # final save JSON (ensures up-to-date)\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(history, f, indent=4)\n",
    "\n",
    "    print(\"\\nTraining complete.\")\n",
    "    print(\"Best checkpoint saved at:\", best_ckpt_path)\n",
    "    print(\"History saved at:\", json_path)\n",
    "\n",
    "    return history, run_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2895275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Run folder: checkpoints\\2025-12-31_18-54-39\n",
      "epoch: 0 | train_total=0.998119 (L1=1.056506, L2=0.903351, end=1.035772) | val_total=0.969549 (L1=1.020336, L2=0.883473, end=1.004122) | lr=1.00e-04\n",
      "epoch: 25 | train_total=0.277744 (L1=0.392116, L2=0.602354, end=0.096377) | val_total=0.276219 (L1=0.389906, L2=0.600194, end=0.095284) | lr=1.00e-04\n",
      "epoch: 50 | train_total=0.234435 (L1=0.294518, L2=0.540593, end=0.071342) | val_total=0.233522 (L1=0.291879, L2=0.539600, end=0.070758) | lr=1.00e-04\n",
      "epoch: 75 | train_total=0.195861 (L1=0.238998, L2=0.512280, end=0.030462) | val_total=0.195520 (L1=0.236697, L2=0.511698, end=0.030568) | lr=1.00e-04\n",
      "epoch: 100 | train_total=0.185279 (L1=0.208021, L2=0.502692, end=0.022783) | val_total=0.184924 (L1=0.206521, L2=0.502277, end=0.022648) | lr=1.00e-04\n",
      "epoch: 125 | train_total=0.178411 (L1=0.192968, L2=0.496571, end=0.016905) | val_total=0.177970 (L1=0.191745, L2=0.496285, end=0.016516) | lr=1.00e-04\n",
      "epoch: 150 | train_total=0.173049 (L1=0.186145, L2=0.491768, end=0.011507) | val_total=0.172854 (L1=0.184995, L2=0.491497, end=0.011509) | lr=1.00e-04\n",
      "epoch: 175 | train_total=0.169552 (L1=0.180370, L2=0.488326, end=0.008362) | val_total=0.169279 (L1=0.179331, L2=0.488107, end=0.008189) | lr=1.00e-04\n",
      "\n",
      "Training complete.\n",
      "Best checkpoint saved at: checkpoints\\2025-12-31_18-54-39\\best.pt\n",
      "History saved at: checkpoints\\2025-12-31_18-54-39\\history.json\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m      7\u001b[39m scheduler = ReduceLROnPlateau(\n\u001b[32m      8\u001b[39m     optimizer,\n\u001b[32m      9\u001b[39m     mode=\u001b[33m\"\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     min_lr=MLR,\n\u001b[32m     15\u001b[39m )\n\u001b[32m     17\u001b[39m history, run_dir = train(\n\u001b[32m     18\u001b[39m     model=model,\n\u001b[32m     19\u001b[39m     train_loader=train_loader,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     save_json_each_epoch=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     29\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest checkpoint saved at:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mbest_path\u001b[49m)\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRun directory:\u001b[39m\u001b[33m\"\u001b[39m, run_dir)\n",
      "\u001b[31mNameError\u001b[39m: name 'best_path' is not defined"
     ]
    }
   ],
   "source": [
    "encoder_net = Encoder().to(device)\n",
    "decoder_net = Decoder().to(device)\n",
    "model = AutoEncoder(encoder_net, decoder_net).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",\n",
    "    factor=0.01,\n",
    "    patience=5,\n",
    "    threshold=1e-4,\n",
    "    cooldown=0,\n",
    "    min_lr=MLR,\n",
    ")\n",
    "\n",
    "history, run_dir = train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    loss_weights=(0.3, 0.1, 0.6),\n",
    "    base_ckpt_dir=\"checkpoints\",\n",
    "    checkpoint_period=200,\n",
    "    print_period=25,\n",
    "    save_json_each_epoch=True,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Best checkpoint saved at:\", best_path)\n",
    "print(\"Run directory:\", run_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9bf566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import math\n",
    "\n",
    "# device setup (equivalent to tf.device('/device:GPU:0'))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Make sure encoder / decoder are already defined and moved to device:\n",
    "# encoder.to(device)\n",
    "# decoder.to(device)\n",
    "# encoder.eval()\n",
    "# decoder.eval()\n",
    "\n",
    "\n",
    "def encode(image, encoder):\n",
    "    \"\"\"\n",
    "    PyTorch version of TensorFlow encode()\n",
    "    Input: image (numpy array), encoder (torch.nn.Module)\n",
    "    Output: pred_maps (numpy), elapsed (float), (WIDTH, HEIGHT)\n",
    "    \"\"\"\n",
    "    print(f\"Image shape at start of encoder method: {image.shape}\")\n",
    "\n",
    "    # Handle 2D flattened input vs image input\n",
    "    if len(image.shape) == 2:\n",
    "        WIDTH = HEIGHT = int(math.sqrt(image.shape[0]))\n",
    "        # your code: reshape(-1,4) then later reshape(W*H,3)\n",
    "        # This implies the 4th channel was ignored later.\n",
    "        # We'll keep it identical:\n",
    "        image = np.asarray(image).reshape(-1, 4).astype(\"float32\")\n",
    "    else:\n",
    "        WIDTH = image.shape[0]\n",
    "        HEIGHT = image.shape[1]\n",
    "        image = np.asarray(image).astype(\"float32\")\n",
    "\n",
    "    # match your Keras reshape\n",
    "    image = image.reshape(WIDTH * HEIGHT, 3)\n",
    "\n",
    "    start = time.time()\n",
    "    print(f\"Image shape before encoder inference: {image.shape}\")\n",
    "\n",
    "    # Convert to torch tensor and run model inference\n",
    "    x = torch.from_numpy(image).to(device)\n",
    "\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_maps = encoder(x)\n",
    "\n",
    "    # Convert back to numpy\n",
    "    pred_maps = pred_maps.detach().cpu().numpy()\n",
    "\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "\n",
    "    # reshape output to (WIDTH*HEIGHT, 5)\n",
    "    pred_maps = pred_maps.reshape(WIDTH * HEIGHT, 5)\n",
    "\n",
    "    return pred_maps, elapsed, (WIDTH, HEIGHT)\n",
    "\n",
    "\n",
    "def decode(encoded, decoder):\n",
    "    \"\"\"\n",
    "    PyTorch version of TensorFlow decode()\n",
    "    Input: encoded (numpy array), decoder (torch.nn.Module)\n",
    "    Output: recovered (numpy), elapsed (float), (WIDTH, HEIGHT)\n",
    "    \"\"\"\n",
    "    print(f\"Image shape going into encoder: {encoded.shape}\")\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Handle 2D flattened input vs (W,H,C) style input\n",
    "    if len(encoded.shape) == 2:\n",
    "        WIDTH = HEIGHT = int(math.sqrt(encoded.shape[0]))\n",
    "        encoded = np.asarray(encoded).reshape(-1, 5).astype(\"float32\")\n",
    "    else:\n",
    "        WIDTH = encoded.shape[0]\n",
    "        HEIGHT = encoded.shape[1]\n",
    "        encoded = np.asarray(encoded).astype(\"float32\")\n",
    "\n",
    "    print(f\"encoded shape going into decoder: {encoded.shape}\")\n",
    "\n",
    "    # Torch inference\n",
    "    x = torch.from_numpy(encoded).to(device)\n",
    "\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        recovered = decoder(x)\n",
    "\n",
    "    recovered = recovered.detach().cpu().numpy()\n",
    "\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "\n",
    "    # reshape output to RGB image\n",
    "    recovered = recovered.reshape(WIDTH, HEIGHT, 3)\n",
    "\n",
    "    return recovered, elapsed, (WIDTH, HEIGHT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb05b49d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9415afa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67153604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc659287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4363a649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4ac264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0fcaef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
