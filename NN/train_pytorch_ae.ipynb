{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ea1afdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b489bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aai\\AppData\\Local\\Temp\\ipykernel_15176\\3831695973.py:4: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(lut_path, names=headers.split(\",\"), header=None)\n"
     ]
    }
   ],
   "source": [
    "headers = \"Cm,Ch,Bm,Bh,T,X,Y,Z,sR,sG,sB\"\n",
    "lut_path = r\"D:\\Github\\PhD Code\\Synthetic Data\\monte_carlo\\lut_rgb_BaseLine.csv\"\n",
    "\n",
    "df = pd.read_csv(lut_path, names=headers.split(\",\"), header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df4c77e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Cm",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Ch",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Bm",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Bh",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "T",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "X",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Y",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Z",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "sR",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "sG",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "sB",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "d160fefb-4406-4b4a-9cbe-31b148aadb98",
       "rows": [
        [
         "0",
         "melanin_concentration(Cm)",
         "blood_concentration(Ch)",
         "melanin_blend(Bm)",
         "BloodOxy",
         "epidermis_thickness(T)",
         "X",
         "Y",
         "Z",
         "sR",
         "sG",
         "sB"
        ],
        [
         "1",
         "0.001",
         "0.001",
         "0",
         "0.627143",
         "0.05",
         "74.714",
         "79.1013",
         "73.3911",
         "236.063",
         "229.876",
         "211.659"
        ],
        [
         "2",
         "0.001",
         "0.001",
         "0",
         "0.627143",
         "0.09",
         "74.8639",
         "79.1686",
         "73.8758",
         "236.238",
         "229.878",
         "212.384"
        ],
        [
         "3",
         "0.001",
         "0.001",
         "0",
         "0.627143",
         "0.17",
         "75.0629",
         "79.3771",
         "74.2289",
         "236.422",
         "230.151",
         "212.871"
        ],
        [
         "4",
         "0.001",
         "0.001",
         "0",
         "0.627143",
         "0.01",
         "74.6183",
         "79.0106",
         "73.1547",
         "235.998",
         "229.764",
         "211.319"
        ],
        [
         "5",
         "0.001",
         "0.001",
         "0",
         "0.627143",
         "0.13",
         "75.0411",
         "79.2762",
         "74.4091",
         "236.415",
         "229.945",
         "213.169"
        ],
        [
         "6",
         "0.001",
         "0.001",
         "0",
         "0.6",
         "0.05",
         "74.7544",
         "79.1063",
         "73.5531",
         "236.116",
         "229.847",
         "211.906"
        ],
        [
         "7",
         "0.001",
         "0.001",
         "0",
         "0.6",
         "0.25",
         "75.2684",
         "79.5495",
         "74.8125",
         "236.558",
         "230.342",
         "213.713"
        ],
        [
         "8",
         "0.001",
         "0.001",
         "0",
         "0.6",
         "0.09",
         "74.8879",
         "79.2206",
         "73.8242",
         "236.267",
         "229.97",
         "212.293"
        ],
        [
         "9",
         "0.001",
         "0.001",
         "0",
         "0.6",
         "0.01",
         "74.6466",
         "79.0374",
         "73.1734",
         "236.049",
         "229.795",
         "211.342"
        ],
        [
         "10",
         "0.001",
         "0.001",
         "0",
         "0.654286",
         "0.05",
         "74.7117",
         "79.092",
         "73.3984",
         "236.067",
         "229.857",
         "211.673"
        ],
        [
         "11",
         "0.001",
         "0.001",
         "0",
         "0.6",
         "0.21",
         "75.1971",
         "79.4678",
         "74.6424",
         "236.532",
         "230.225",
         "213.476"
        ],
        [
         "12",
         "0.001",
         "0.001",
         "0",
         "0.6",
         "0.17",
         "75.0746",
         "79.4017",
         "74.328",
         "236.361",
         "230.201",
         "213.014"
        ],
        [
         "13",
         "0.001",
         "0.001",
         "0",
         "0.6",
         "0.13",
         "75.0076",
         "79.2614",
         "74.2077",
         "236.433",
         "229.94",
         "212.868"
        ],
        [
         "14",
         "0.001",
         "0.001",
         "0",
         "0.654286",
         "0.01",
         "74.5706",
         "78.8994",
         "72.9939",
         "236.118",
         "229.546",
         "211.104"
        ],
        [
         "15",
         "0.001",
         "0.001",
         "0",
         "0.627143",
         "0.25",
         "75.2821",
         "79.4928",
         "74.7805",
         "236.741",
         "230.187",
         "213.682"
        ],
        [
         "16",
         "0.001",
         "0.001",
         "0",
         "0.627143",
         "0.21",
         "75.2086",
         "79.4032",
         "74.7421",
         "236.64",
         "230.06",
         "213.645"
        ],
        [
         "17",
         "0.001",
         "0.001",
         "0",
         "0.654286",
         "0.13",
         "75.0012",
         "79.2037",
         "74.3135",
         "236.452",
         "229.815",
         "213.044"
        ],
        [
         "18",
         "0.001",
         "0.001",
         "0",
         "0.681429",
         "0.05",
         "74.7407",
         "79.0122",
         "73.6019",
         "236.21",
         "229.639",
         "212.006"
        ],
        [
         "19",
         "0.001",
         "0.001",
         "0",
         "0.681429",
         "0.01",
         "74.5812",
         "78.9347",
         "73.0963",
         "236.029",
         "229.624",
         "211.25"
        ],
        [
         "20",
         "0.001",
         "0.001",
         "0",
         "0.654286",
         "0.09",
         "74.8499",
         "79.1013",
         "73.9051",
         "236.292",
         "229.734",
         "212.447"
        ],
        [
         "21",
         "0.001",
         "0.001",
         "0",
         "0.654286",
         "0.17",
         "75.0638",
         "79.3111",
         "74.3463",
         "236.479",
         "229.997",
         "213.067"
        ],
        [
         "22",
         "0.001",
         "0.001",
         "0",
         "0.681429",
         "0.17",
         "75.0204",
         "79.2556",
         "74.3054",
         "236.435",
         "229.915",
         "213.018"
        ],
        [
         "23",
         "0.001",
         "0.001",
         "0",
         "0.681429",
         "0.21",
         "75.1759",
         "79.3443",
         "74.7971",
         "236.587",
         "229.962",
         "213.742"
        ],
        [
         "24",
         "0.001",
         "0.001",
         "0",
         "0.681429",
         "0.13",
         "75.0148",
         "79.2048",
         "74.3761",
         "236.466",
         "229.804",
         "213.138"
        ],
        [
         "25",
         "0.001",
         "0.001",
         "0",
         "0.654286",
         "0.25",
         "75.2617",
         "79.462",
         "74.9066",
         "236.639",
         "230.145",
         "213.878"
        ],
        [
         "26",
         "0.001",
         "0.001",
         "0",
         "0.681429",
         "0.09",
         "74.8078",
         "79.0573",
         "73.8143",
         "236.263",
         "229.676",
         "212.319"
        ],
        [
         "27",
         "0.001",
         "0.001",
         "0",
         "0.654286",
         "0.21",
         "75.231",
         "79.3886",
         "74.9196",
         "236.648",
         "230.007",
         "213.916"
        ],
        [
         "28",
         "0.001",
         "0.001",
         "0",
         "0.681429",
         "0.25",
         "75.2322",
         "79.423",
         "74.9177",
         "236.589",
         "230.088",
         "213.904"
        ],
        [
         "29",
         "0.001",
         "0.001",
         "0",
         "0.708571",
         "0.09",
         "74.8219",
         "79.0573",
         "73.92",
         "236.254",
         "229.664",
         "212.48"
        ],
        [
         "30",
         "0.001",
         "0.001",
         "0",
         "0.708571",
         "0.01",
         "74.5425",
         "78.9075",
         "73.0048",
         "235.982",
         "229.602",
         "211.116"
        ],
        [
         "31",
         "0.001",
         "0.001",
         "0",
         "0.708571",
         "0.05",
         "74.7113",
         "78.9962",
         "73.5693",
         "236.143",
         "229.636",
         "211.959"
        ],
        [
         "32",
         "0.001",
         "0.001",
         "0",
         "0.708571",
         "0.13",
         "74.9572",
         "79.1645",
         "74.2848",
         "236.368",
         "229.773",
         "213.008"
        ],
        [
         "33",
         "0.001",
         "0.001",
         "0",
         "0.708571",
         "0.17",
         "75.0008",
         "79.2566",
         "74.2621",
         "236.381",
         "229.94",
         "212.951"
        ],
        [
         "34",
         "0.001",
         "0.001",
         "0",
         "0.708571",
         "0.21",
         "75.1395",
         "79.3272",
         "74.7011",
         "236.533",
         "229.961",
         "213.6"
        ],
        [
         "35",
         "0.001",
         "0.001",
         "0",
         "0.735714",
         "0.01",
         "74.5503",
         "78.8944",
         "73.1006",
         "235.98",
         "229.565",
         "211.266"
        ],
        [
         "36",
         "0.001",
         "0.001",
         "0",
         "0.708571",
         "0.25",
         "75.2186",
         "79.4105",
         "74.9034",
         "236.566",
         "230.074",
         "213.885"
        ],
        [
         "37",
         "0.001",
         "0.001",
         "0",
         "0.735714",
         "0.05",
         "74.713",
         "78.999",
         "73.6242",
         "236.11",
         "229.643",
         "212.041"
        ],
        [
         "38",
         "0.001",
         "0.001",
         "0",
         "0.735714",
         "0.17",
         "75.0061",
         "79.2538",
         "74.3567",
         "236.35",
         "229.932",
         "213.094"
        ],
        [
         "39",
         "0.001",
         "0.001",
         "0",
         "0.735714",
         "0.21",
         "75.1139",
         "79.3109",
         "74.6669",
         "236.482",
         "229.952",
         "213.552"
        ],
        [
         "40",
         "0.001",
         "0.001",
         "0",
         "0.735714",
         "0.09",
         "74.8119",
         "79.0471",
         "73.9783",
         "236.197",
         "229.655",
         "212.57"
        ],
        [
         "41",
         "0.001",
         "0.001",
         "0",
         "0.735714",
         "0.13",
         "74.9583",
         "79.148",
         "74.4471",
         "236.303",
         "229.741",
         "213.257"
        ],
        [
         "42",
         "0.001",
         "0.001",
         "0",
         "0.762857",
         "0.01",
         "74.4928",
         "78.8248",
         "72.9929",
         "235.947",
         "229.463",
         "211.118"
        ],
        [
         "43",
         "0.001",
         "0.001",
         "0",
         "0.735714",
         "0.25",
         "75.2137",
         "79.4211",
         "74.8491",
         "236.56",
         "230.103",
         "213.8"
        ],
        [
         "44",
         "0.001",
         "0.001",
         "0",
         "0.762857",
         "0.05",
         "74.6948",
         "78.9724",
         "73.5684",
         "236.122",
         "229.599",
         "211.963"
        ],
        [
         "45",
         "0.001",
         "0.001",
         "0",
         "0.762857",
         "0.13",
         "74.9177",
         "79.1319",
         "74.3489",
         "236.231",
         "229.747",
         "213.111"
        ],
        [
         "46",
         "0.001",
         "0.001",
         "0",
         "0.762857",
         "0.09",
         "74.7582",
         "79.0101",
         "73.8728",
         "236.117",
         "229.627",
         "212.417"
        ],
        [
         "47",
         "0.001",
         "0.001",
         "0",
         "0.762857",
         "0.17",
         "74.9876",
         "79.2309",
         "74.3441",
         "236.327",
         "229.899",
         "213.081"
        ],
        [
         "48",
         "0.001",
         "0.001",
         "0",
         "0.762857",
         "0.21",
         "75.0763",
         "79.2755",
         "74.6255",
         "236.424",
         "229.911",
         "213.497"
        ],
        [
         "49",
         "0.001",
         "0.001",
         "0",
         "0.762857",
         "0.25",
         "75.213",
         "79.4127",
         "74.9297",
         "236.524",
         "230.088",
         "213.923"
        ]
       ],
       "shape": {
        "columns": 11,
        "rows": 840001
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cm</th>\n",
       "      <th>Ch</th>\n",
       "      <th>Bm</th>\n",
       "      <th>Bh</th>\n",
       "      <th>T</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "      <th>sR</th>\n",
       "      <th>sG</th>\n",
       "      <th>sB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>melanin_concentration(Cm)</td>\n",
       "      <td>blood_concentration(Ch)</td>\n",
       "      <td>melanin_blend(Bm)</td>\n",
       "      <td>BloodOxy</td>\n",
       "      <td>epidermis_thickness(T)</td>\n",
       "      <td>X</td>\n",
       "      <td>Y</td>\n",
       "      <td>Z</td>\n",
       "      <td>sR</td>\n",
       "      <td>sG</td>\n",
       "      <td>sB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>0.627143</td>\n",
       "      <td>0.05</td>\n",
       "      <td>74.714</td>\n",
       "      <td>79.1013</td>\n",
       "      <td>73.3911</td>\n",
       "      <td>236.063</td>\n",
       "      <td>229.876</td>\n",
       "      <td>211.659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>0.627143</td>\n",
       "      <td>0.09</td>\n",
       "      <td>74.8639</td>\n",
       "      <td>79.1686</td>\n",
       "      <td>73.8758</td>\n",
       "      <td>236.238</td>\n",
       "      <td>229.878</td>\n",
       "      <td>212.384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>0.627143</td>\n",
       "      <td>0.17</td>\n",
       "      <td>75.0629</td>\n",
       "      <td>79.3771</td>\n",
       "      <td>74.2289</td>\n",
       "      <td>236.422</td>\n",
       "      <td>230.151</td>\n",
       "      <td>212.871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>0.627143</td>\n",
       "      <td>0.01</td>\n",
       "      <td>74.6183</td>\n",
       "      <td>79.0106</td>\n",
       "      <td>73.1547</td>\n",
       "      <td>235.998</td>\n",
       "      <td>229.764</td>\n",
       "      <td>211.319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839996</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1.6121</td>\n",
       "      <td>1.5696</td>\n",
       "      <td>1.20872</td>\n",
       "      <td>40.91</td>\n",
       "      <td>31.841</td>\n",
       "      <td>26.229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839997</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.43685</td>\n",
       "      <td>1.45939</td>\n",
       "      <td>1.237</td>\n",
       "      <td>36.375</td>\n",
       "      <td>31.36</td>\n",
       "      <td>26.903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839998</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.47819</td>\n",
       "      <td>1.48677</td>\n",
       "      <td>1.23741</td>\n",
       "      <td>37.431</td>\n",
       "      <td>31.514</td>\n",
       "      <td>26.859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839999</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.952857</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2.22331</td>\n",
       "      <td>1.86828</td>\n",
       "      <td>1.28201</td>\n",
       "      <td>54.037</td>\n",
       "      <td>31.454</td>\n",
       "      <td>27.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840000</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.27455</td>\n",
       "      <td>3.2389</td>\n",
       "      <td>2.43232</td>\n",
       "      <td>78.211</td>\n",
       "      <td>39.065</td>\n",
       "      <td>40.279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>840001 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Cm                       Ch                 Bm  \\\n",
       "0       melanin_concentration(Cm)  blood_concentration(Ch)  melanin_blend(Bm)   \n",
       "1                           0.001                    0.001                  0   \n",
       "2                           0.001                    0.001                  0   \n",
       "3                           0.001                    0.001                  0   \n",
       "4                           0.001                    0.001                  0   \n",
       "...                           ...                      ...                ...   \n",
       "839996                        0.5                     0.32                1.0   \n",
       "839997                        0.5                     0.32                1.0   \n",
       "839998                        0.5                     0.32                1.0   \n",
       "839999                        0.5                     0.32                1.0   \n",
       "840000                        0.5                     0.32                1.0   \n",
       "\n",
       "              Bh                       T        X        Y        Z       sR  \\\n",
       "0       BloodOxy  epidermis_thickness(T)        X        Y        Z       sR   \n",
       "1       0.627143                    0.05   74.714  79.1013  73.3911  236.063   \n",
       "2       0.627143                    0.09  74.8639  79.1686  73.8758  236.238   \n",
       "3       0.627143                    0.17  75.0629  79.3771  74.2289  236.422   \n",
       "4       0.627143                    0.01  74.6183  79.0106  73.1547  235.998   \n",
       "...          ...                     ...      ...      ...      ...      ...   \n",
       "839996      0.98                    0.13   1.6121   1.5696  1.20872    40.91   \n",
       "839997      0.98                    0.25  1.43685  1.45939    1.237   36.375   \n",
       "839998      0.98                    0.21  1.47819  1.48677  1.23741   37.431   \n",
       "839999  0.952857                    0.05  2.22331  1.86828  1.28201   54.037   \n",
       "840000      0.98                    0.01  4.27455   3.2389  2.43232   78.211   \n",
       "\n",
       "             sG       sB  \n",
       "0            sG       sB  \n",
       "1       229.876  211.659  \n",
       "2       229.878  212.384  \n",
       "3       230.151  212.871  \n",
       "4       229.764  211.319  \n",
       "...         ...      ...  \n",
       "839996   31.841   26.229  \n",
       "839997    31.36   26.903  \n",
       "839998   31.514   26.859  \n",
       "839999   31.454   27.028  \n",
       "840000   39.065   40.279  \n",
       "\n",
       "[840001 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a20f50b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Cm     Ch Bm        Bh     T        X        Y        Z       sR  \\\n",
      "1  0.001  0.001  0  0.627143  0.05   74.714  79.1013  73.3911  236.063   \n",
      "2  0.001  0.001  0  0.627143  0.09  74.8639  79.1686  73.8758  236.238   \n",
      "3  0.001  0.001  0  0.627143  0.17  75.0629  79.3771  74.2289  236.422   \n",
      "4  0.001  0.001  0  0.627143  0.01  74.6183  79.0106  73.1547  235.998   \n",
      "5  0.001  0.001  0  0.627143  0.13  75.0411  79.2762  74.4091  236.415   \n",
      "\n",
      "        sG       sB  \n",
      "1  229.876  211.659  \n",
      "2  229.878  212.384  \n",
      "3  230.151  212.871  \n",
      "4  229.764  211.319  \n",
      "5  229.945  213.169  \n",
      "840000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aai\\AppData\\Local\\Temp\\ipykernel_15176\\3995564710.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = pd.to_numeric(df[column], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Cm     Ch   Bm        Bh     T        X        Y        Z     sR     sG  \\\n",
      "1  0.001  0.001  0.0  0.627143  0.05  74.7140  79.1013  73.3911  236.0  230.0   \n",
      "2  0.001  0.001  0.0  0.627143  0.09  74.8639  79.1686  73.8758  236.0  230.0   \n",
      "3  0.001  0.001  0.0  0.627143  0.17  75.0629  79.3771  74.2289  236.0  230.0   \n",
      "4  0.001  0.001  0.0  0.627143  0.01  74.6183  79.0106  73.1547  236.0  230.0   \n",
      "5  0.001  0.001  0.0  0.627143  0.13  75.0411  79.2762  74.4091  236.0  230.0   \n",
      "\n",
      "      sB  \n",
      "1  212.0  \n",
      "2  212.0  \n",
      "3  213.0  \n",
      "4  211.0  \n",
      "5  213.0  \n",
      "length of df 958003\n",
      "bef norm x_train[0] [159.612 120.733 127.799]\n",
      "aft norm x_train[0] [0.6259294  0.47346276 0.50117254]\n",
      "length of x_train 766401\n",
      "length of x_test 191600\n",
      "length of y_train 766401\n",
      "length of y_test 191600\n",
      "length of df 958003\n",
      "Cm = [np.float64(0.001), np.float64(0.00239541), np.float64(0.00439078), np.float64(0.00698613), np.float64(0.0101815), np.float64(0.0139767), np.float64(0.018372), np.float64(0.0233672), np.float64(0.0289625), np.float64(0.0351576), np.float64(0.0419528), np.float64(0.0493479), np.float64(0.057343), np.float64(0.0659381), np.float64(0.0751331), np.float64(0.0849281), np.float64(0.0953231), np.float64(0.106318), np.float64(0.117913), np.float64(0.130108), np.float64(0.142903), np.float64(0.156298), np.float64(0.170292), np.float64(0.184887), np.float64(0.200082), np.float64(0.215877), np.float64(0.232271), np.float64(0.249266), np.float64(0.266861), np.float64(0.285055), np.float64(0.30385), np.float64(0.323245), np.float64(0.343239), np.float64(0.363834), np.float64(0.385028), np.float64(0.406822), np.float64(0.429217), np.float64(0.452211), np.float64(0.475806), np.float64(0.5)]\n",
      "Ch = [np.float64(0.001), np.float64(0.0020536), np.float64(0.00348225), np.float64(0.00528595), np.float64(0.00746469), np.float64(0.0100185), np.float64(0.0129473), np.float64(0.0162512), np.float64(0.0199301), np.float64(0.0239841), np.float64(0.0284131), np.float64(0.0332172), np.float64(0.0383963), np.float64(0.0439505), np.float64(0.0498797), np.float64(0.0561839), np.float64(0.0628632), np.float64(0.0699176), np.float64(0.077347), np.float64(0.0851514), np.float64(0.0933309), np.float64(0.101885), np.float64(0.110815), np.float64(0.12012), np.float64(0.129799), np.float64(0.139854), np.float64(0.150284), np.float64(0.161089), np.float64(0.172268), np.float64(0.183823), np.float64(0.195753), np.float64(0.208058), np.float64(0.220738), np.float64(0.233793), np.float64(0.247224), np.float64(0.261029), np.float64(0.275209), np.float64(0.289764), np.float64(0.304695), np.float64(0.32)]\n",
      "Bm = [np.float64(0.0), np.float64(0.0625), np.float64(0.25), np.float64(0.5625), np.float64(1.0)]\n",
      "Bh = [np.float64(0.6), np.float64(0.627143), np.float64(0.654286), np.float64(0.681429), np.float64(0.708571), np.float64(0.735714), np.float64(0.762857), np.float64(0.79), np.float64(0.817143), np.float64(0.844286), np.float64(0.871429), np.float64(0.898571), np.float64(0.925714), np.float64(0.952857), np.float64(0.98)]\n",
      "T = [np.float64(0.01), np.float64(0.05), np.float64(0.09), np.float64(0.13), np.float64(0.17), np.float64(0.21), np.float64(0.25)]\n",
      "upper bounds = [np.float64(0.5), np.float64(0.32), np.float64(1.0), np.float64(0.98), np.float64(0.25)]\n",
      "lower bounds = [np.float64(0.001), np.float64(0.001), np.float64(0.0), np.float64(0.6), np.float64(0.01)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aai\\AppData\\Local\\Temp\\ipykernel_15176\\3995564710.py:106: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[['sR', 'sG', 'sB']] = df[['sR', 'sG', 'sB']].applymap(np.round)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of repeated RGB values 930979\n"
     ]
    }
   ],
   "source": [
    "upper_bounds = [0.5, 0.32, 1.0, 0.98, 0.25]\n",
    "lower_bounds = [0.001, 0.001, 0.0, 0.6, 0.01]\n",
    "\n",
    "# Pair up the corresponding bounds and calculate their average\n",
    "averages = [(u + l) / 2 for u, l in zip(upper_bounds, lower_bounds)]\n",
    "avg_Cm, avg_Ch, avg_Bm, avg_Bh, avg_T = averages\n",
    "\n",
    "\n",
    "#remove row 0\n",
    "df = df.iloc[1:]\n",
    "print(df.head())\n",
    "\n",
    "#print length of df\n",
    "print(len(df))\n",
    "# convert all columns to float\n",
    "for column in df.columns:\n",
    "    df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "#convert sR, sG, sB to int\n",
    "rounded_df = df.round({'sR': 0, 'sG': 0, 'sB': 0})\n",
    "# Step 1: Find duplicate RGB values\n",
    "duplicates = rounded_df[rounded_df.duplicated(subset=['sR', 'sG', 'sB'], keep=False)].copy() # added .copy()\n",
    "#print all duplicates\n",
    "print(duplicates.head())\n",
    "\n",
    "# Step 2: Calculate the 'likelihood' score for each row\n",
    "duplicates['likelihood'] = (abs(duplicates['Cm'] - avg_Cm) +\n",
    "                            abs(duplicates['Ch'] - avg_Ch) +\n",
    "                            abs(duplicates['Bm'] - avg_Bm) +\n",
    "                            abs(duplicates['Bh'] - avg_Bh) +\n",
    "                            abs(duplicates['T'] - avg_T))\n",
    "\n",
    "# Step 3: Sort by RGB values and likelihood, keeping the row with the lowest likelihood for each RGB group\n",
    "most_likely_duplicates = duplicates.sort_values(['sR', 'sG', 'sB', 'likelihood']).drop_duplicates(subset=['sR', 'sG', 'sB'])\n",
    "\n",
    "# Now, most_likely_duplicates should contain your desired rows\n",
    "\n",
    "# First, remove all duplicates from the original dataframe\n",
    "df_no_duplicates = df.drop_duplicates(subset=['sR', 'sG', 'sB'], keep=False)\n",
    "\n",
    "# Concatenate df_no_duplicates with most_likely_duplicates to get the final dataframe\n",
    "df = pd.concat([df_no_duplicates, most_likely_duplicates])\n",
    "\n",
    "# If you want to sort it based on index\n",
    "df.sort_index(inplace=True)\n",
    "df.head()\n",
    "#remove duplicates\n",
    "\n",
    "x = df[['sR', 'sG', 'sB']].to_numpy(dtype='float32')\n",
    "y = df[['Cm', 'Ch', 'Bm', 'Bh', 'T']].to_numpy(dtype='float32')\n",
    "#create new csv with headers\n",
    "# df.to_csv(r'LUTs\\large_no_duplicates.csv', index=False, header=True)\n",
    "\n",
    "\n",
    "#train nn on x,y\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "#remove any header values\n",
    "x_train = x_train[1:]\n",
    "x_test = x_test[1:]\n",
    "y_train = y_train[1:]\n",
    "y_test = y_test[1:]\n",
    "\n",
    "#numpy arrays\n",
    "x_train = np.asarray(x_train).reshape(-1,3).astype('float32')\n",
    "x_test = np.asarray(x_test).reshape(-1,3).astype('float32')\n",
    "print(f\"length of df {len(df)}\")\n",
    "print(f\"bef norm x_train[0] {x_train[0]}\")\n",
    "\n",
    "#normalize\n",
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0\n",
    "print(f\"aft norm x_train[0] {x_train[0]}\")\n",
    "\n",
    "print(f\"length of x_train {len(x_train)}\")\n",
    "print(f\"length of x_test {len(x_test)}\")\n",
    "print(f\"length of y_train {len(y_train)}\")\n",
    "print(f\"length of y_test {len(y_test)}\")\n",
    "df.head()\n",
    "print(f\"length of df {len(df)}\")\n",
    "#print random 3 rows\n",
    "#print unique values of Cm,Ch,Bm,Bh,T\n",
    "# print(f\"unique Cm {df['Cm'].unique()}\")\n",
    "# print(f\"unique Ch {df['Ch'].unique()}\")\n",
    "# print(f\"unique Bm {df['Bm'].unique()}\")\n",
    "# print(f\"unique Bh {df['Bh'].unique()}\")\n",
    "# print(f\"unique T {df['T'].unique()}\")\n",
    "#as sorted lists\n",
    "C_m = sorted(df['Cm'].unique())\n",
    "C_h = sorted(df['Ch'].unique())\n",
    "B_m = sorted(df['Bm'].unique())\n",
    "B_h = sorted(df['Bh'].unique())\n",
    "T = sorted(df['T'].unique())\n",
    "print(f\"Cm = {C_m}\")\n",
    "print(f\"Ch = {C_h}\")\n",
    "print(f\"Bm = {B_m}\")\n",
    "print(f\"Bh = {B_h}\")\n",
    "print(f\"T = {T}\")\n",
    "#min max for each\n",
    "min_vals = [min(C_m), min(C_h), min(B_m), min(B_h), min(T)]\n",
    "max_vals = [max(C_m), max(C_h), max(B_m), max(B_h), max(T)]\n",
    "print(f\"upper bounds = {max_vals}\")\n",
    "print(f\"lower bounds = {min_vals}\")\n",
    "#integer arrays for sR,sG,sB 0 to 255\n",
    "# Assuming df is your DataFrame and it has columns 'sR', 'sG', 'sB'\n",
    "df[['sR', 'sG', 'sB']] = df[['sR', 'sG', 'sB']].astype(float)\n",
    "df[['sR', 'sG', 'sB']] = df[['sR', 'sG', 'sB']].applymap(np.round)\n",
    "# Add a 'count' column that counts the number of identical RGB values\n",
    "df['count'] = df.groupby(['sR', 'sG', 'sB'])['sR'].transform('count')\n",
    "print(f\"number of repeated RGB values {len(df[df['count'] > 1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b922472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Reproducibility (np.random.seed(7) equivalent)\n",
    "# -----------------------------\n",
    "np.random.seed(7)\n",
    "torch.manual_seed(7)\n",
    "torch.cuda.manual_seed_all(7)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -----------------------------\n",
    "# Hyperparameters (from your code)\n",
    "# -----------------------------\n",
    "BATCH_SIZE = 4096 * 16\n",
    "NUM_NEURONS = 256\n",
    "NUM_LAYERS = 8\n",
    "NUM_EPOCHS = 200\n",
    "LR = 1e-4\n",
    "MLR = 1e-6  # unused in your compile; keeping for parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "848fb777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking train_loader...\n",
      "Batch contains 6 tensors\n",
      "  Item 0: shape = torch.Size([65536, 3]), type = <class 'torch.Tensor'>\n",
      "  Item 1: shape = torch.Size([65536, 5]), type = <class 'torch.Tensor'>\n",
      "  Item 2: shape = torch.Size([65536, 3]), type = <class 'torch.Tensor'>\n",
      "  Item 3: shape = torch.Size([65536, 5]), type = <class 'torch.Tensor'>\n",
      "  Item 4: shape = torch.Size([65536, 3]), type = <class 'torch.Tensor'>\n",
      "  Item 5: shape = torch.Size([65536, 3]), type = <class 'torch.Tensor'>\n",
      "\n",
      "Checking val_loader...\n",
      "Batch contains 6 tensors\n",
      "  Item 0: shape = torch.Size([65536, 3]), type = <class 'torch.Tensor'>\n",
      "  Item 1: shape = torch.Size([65536, 5]), type = <class 'torch.Tensor'>\n",
      "  Item 2: shape = torch.Size([65536, 3]), type = <class 'torch.Tensor'>\n",
      "  Item 3: shape = torch.Size([65536, 5]), type = <class 'torch.Tensor'>\n",
      "  Item 4: shape = torch.Size([65536, 3]), type = <class 'torch.Tensor'>\n",
      "  Item 5: shape = torch.Size([65536, 3]), type = <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class AEDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.from_numpy(x).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x_i = self.x[idx]   # [3]\n",
    "        y_i = self.y[idx]   # [5]\n",
    "        \n",
    "        enc_in = x_i\n",
    "        dec_in = y_i\n",
    "        end_in = x_i\n",
    "        \n",
    "        enc_true = y_i\n",
    "        dec_true = x_i\n",
    "        end_true = x_i\n",
    "        \n",
    "        return enc_in, dec_in, end_in, enc_true, dec_true, end_true\n",
    "\n",
    "\n",
    "# Create train and validation datasets\n",
    "train_dataset = AEDataset(x_train, y_train)\n",
    "val_dataset = AEDataset(x_test, y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# DEBUG: Let's see what the dataloader is actually giving us\n",
    "print(\"Checking train_loader...\")\n",
    "for batch in train_loader:\n",
    "    print(f\"Batch contains {len(batch)} tensors\")\n",
    "    for i, item in enumerate(batch):\n",
    "        print(f\"  Item {i}: shape = {item.shape if hasattr(item, 'shape') else 'NO SHAPE'}, type = {type(item)}\")\n",
    "    break  # Just check first batch\n",
    "\n",
    "print(\"\\nChecking val_loader...\")\n",
    "for batch in val_loader:\n",
    "    print(f\"Batch contains {len(batch)} tensors\")\n",
    "    for i, item in enumerate(batch):\n",
    "        print(f\"  Item {i}: shape = {item.shape if hasattr(item, 'shape') else 'NO SHAPE'}, type = {type(item)}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95f9fb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def show_training_progress(encoder, decoder, x_batch, y_batch, device, epoch, save_dir):\n",
    "    \"\"\"\n",
    "    Shows RGB → Encoder → Latent → Decoder → RGB\n",
    "    Saves visualization to file and returns metrics\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        latent_predicted = encoder(x_batch[:3])\n",
    "        rgb_output = decoder(latent_predicted)\n",
    "    \n",
    "    # Convert to readable format\n",
    "    rgb_input = (x_batch[:3].cpu().numpy() * 255).astype(int)\n",
    "    latent_pred = latent_predicted.cpu().numpy()\n",
    "    latent_true = y_batch[:3].cpu().numpy()\n",
    "    rgb_recon = (rgb_output.cpu().numpy() * 255).astype(int)\n",
    "    \n",
    "    # Create plot\n",
    "    fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
    "    fig.suptitle(f'Epoch {epoch} - Training Progress', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    params = ['Cm', 'Ch', 'Bm', 'Bh', 'T']\n",
    "    \n",
    "    for i in range(3):\n",
    "        # Input color\n",
    "        axes[i,0].imshow([[rgb_input[i]/255.0]])\n",
    "        axes[i,0].set_title(f'Input\\n{rgb_input[i]}')\n",
    "        axes[i,0].axis('off')\n",
    "        \n",
    "        # True parameters\n",
    "        axes[i,1].barh(params, latent_true[i], color='green', alpha=0.6)\n",
    "        axes[i,1].set_xlim(0,1)\n",
    "        axes[i,1].set_title('True Parameters')\n",
    "        \n",
    "        # Predicted parameters\n",
    "        axes[i,2].barh(params, latent_pred[i], color='orange', alpha=0.6)\n",
    "        axes[i,2].set_xlim(0,1)\n",
    "        axes[i,2].set_title('Encoder Output')\n",
    "        \n",
    "        # Reconstructed color\n",
    "        axes[i,3].imshow([[rgb_recon[i]/255.0]])\n",
    "        axes[i,3].set_title(f'Output\\n{rgb_recon[i]}')\n",
    "        axes[i,3].axis('off')\n",
    "        \n",
    "        # Error\n",
    "        error = np.abs(rgb_input[i] - rgb_recon[i])\n",
    "        axes[i,4].axis('off')\n",
    "        axes[i,4].text(0.1, 0.5, f'Error:\\n{error}\\n\\nAvg: {error.mean():.1f}',\n",
    "                      fontsize=10, bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # SAVE to file\n",
    "    save_path = os.path.join(save_dir, f'epoch_{epoch:04d}.png')\n",
    "    plt.savefig(save_path, dpi=100, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Return metrics for logging\n",
    "    metrics = []\n",
    "    for i in range(3):\n",
    "        error = np.abs(rgb_input[i] - rgb_recon[i])\n",
    "        metrics.append({\n",
    "            'sample': i,\n",
    "            'rgb_input': rgb_input[i].tolist(),\n",
    "            'rgb_output': rgb_recon[i].tolist(),\n",
    "            'mean_error': float(error.mean())\n",
    "        })\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f934a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import json\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# DEVICE\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# MODEL COMPONENTS\n",
    "# -----------------------------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim=3, hidden_dim=NUM_NEURONS, num_layers=NUM_LAYERS, out_dim=5):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Linear(in_dim if i == 0 else hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.out = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_dim=5, hidden_dim=NUM_NEURONS, num_layers=NUM_LAYERS, out_dim=3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Linear(in_dim if i == 0 else hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.out = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, encoder: nn.Module, decoder: nn.Module):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, encoder_in, decoder_in, end_to_end_in):\n",
    "        enc_out = self.encoder(encoder_in)\n",
    "        dec_out = self.decoder(decoder_in)\n",
    "        end_out = self.decoder(self.encoder(end_to_end_in))\n",
    "        return enc_out, dec_out, end_out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Loss functions (same as before)\n",
    "# -----------------------------\n",
    "def albedo_loss(y_true, y_pred):\n",
    "    return torch.sum(torch.abs(y_pred - y_true), dim=-1)\n",
    "\n",
    "def parameter_loss(y_true, y_pred):\n",
    "    return torch.sqrt(torch.sum((y_pred - y_true) ** 2, dim=-1) + 1e-12)\n",
    "\n",
    "def end_to_end_loss(y_true, y_pred):\n",
    "    return torch.sum(torch.abs(y_pred - y_true), dim=-1)\n",
    "\n",
    "def reduce_loss(loss_per_sample, reduction=\"mean\"):\n",
    "    if reduction == \"mean\":\n",
    "        return loss_per_sample.mean()\n",
    "    elif reduction == \"sum\":\n",
    "        return loss_per_sample.sum()\n",
    "    else:\n",
    "        raise ValueError(\"reduction must be 'mean' or 'sum'\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Run folder creation (date/time)\n",
    "# -----------------------------\n",
    "def create_run_folder(base_dir=\"checkpoints\"):\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    run_dir = os.path.join(base_dir, now)\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    return run_dir\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation loop (val)\n",
    "# -----------------------------\n",
    "def evaluate(model, dataloader, loss_weights=(0.3, 0.1, 0.6)):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss_sum = 0.0\n",
    "    loss1_sum = 0.0\n",
    "    loss2_sum = 0.0\n",
    "    loss3_sum = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            enc_in, dec_in, end_in, enc_true, dec_true, end_true = batch\n",
    "\n",
    "            enc_in   = enc_in.to(device)\n",
    "            dec_in   = dec_in.to(device)\n",
    "            end_in   = end_in.to(device)\n",
    "            enc_true = enc_true.to(device)\n",
    "            dec_true = dec_true.to(device)\n",
    "            end_true = end_true.to(device)\n",
    "\n",
    "            enc_pred, dec_pred, end_pred = model(enc_in, dec_in, end_in)\n",
    "\n",
    "            loss1 = reduce_loss(parameter_loss(enc_true, enc_pred), \"mean\")\n",
    "            loss2 = reduce_loss(albedo_loss(dec_true, dec_pred), \"mean\")\n",
    "            loss3 = reduce_loss(end_to_end_loss(end_true, end_pred), \"mean\")\n",
    "\n",
    "            total_loss = loss_weights[0]*loss1 + loss_weights[1]*loss2 + loss_weights[2]*loss3\n",
    "\n",
    "            total_loss_sum += total_loss.item()\n",
    "            loss1_sum += loss1.item()\n",
    "            loss2_sum += loss2.item()\n",
    "            loss3_sum += loss3.item()\n",
    "            n_batches += 1\n",
    "\n",
    "    if n_batches == 0:\n",
    "        return {\"total\": 0.0, \"loss1\": 0.0, \"loss2\": 0.0, \"loss3\": 0.0}\n",
    "\n",
    "    return {\n",
    "        \"total\": total_loss_sum / n_batches,\n",
    "        \"loss1\": loss1_sum / n_batches,\n",
    "        \"loss2\": loss2_sum / n_batches,\n",
    "        \"loss3\": loss3_sum / n_batches,\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training loop (train + val)\n",
    "# -----------------------------\n",
    "def train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    num_epochs=200,\n",
    "    loss_weights=(0.3, 0.1, 0.6),\n",
    "    base_ckpt_dir=\"checkpoints\",\n",
    "    checkpoint_period=200,\n",
    "    print_period=25,\n",
    "    save_json_each_epoch=True,\n",
    "):\n",
    "    run_dir = create_run_folder(base_ckpt_dir)\n",
    "    best_ckpt_path = os.path.join(run_dir, \"best.pt\")\n",
    "    last_ckpt_path = os.path.join(run_dir, \"last.pt\")\n",
    "    json_path = os.path.join(run_dir, \"history.json\")\n",
    "\n",
    "    print(\"Using device:\", device)\n",
    "    print(\"Run folder:\", run_dir)\n",
    "\n",
    "    # CREATE VISUALIZATION FOLDER\n",
    "    viz_dir = os.path.join(run_dir, \"visualizations\")\n",
    "    os.makedirs(viz_dir, exist_ok=True)\n",
    "    \n",
    "    # CREATE LOG FILE\n",
    "    log_file_path = os.path.join(viz_dir, \"training_log.txt\")\n",
    "    with open(log_file_path, 'w') as f:\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"AUTOENCODER TRAINING LOG\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(f\"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Device: {device}\\n\")\n",
    "        f.write(f\"Number of Epochs: {num_epochs}\\n\")\n",
    "        f.write(f\"Batch Size: {train_loader.batch_size}\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "    print(\"Using device:\", device)\n",
    "    print(\"Run folder:\", run_dir)\n",
    "    print(\"Visualizations folder:\", viz_dir)\n",
    "    print(\"Training log:\", log_file_path)\n",
    "\n",
    "    best_train_loss = float(\"inf\")\n",
    "\n",
    "    history = {\n",
    "        \"config\": {\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"loss_weights\": loss_weights,\n",
    "            \"optimizer\": optimizer.__class__.__name__,\n",
    "            \"scheduler\": scheduler.__class__.__name__,\n",
    "            \"device\": str(device),\n",
    "        },\n",
    "        \"epochs\": []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        total_loss_sum = 0.0\n",
    "        loss1_sum = 0.0\n",
    "        loss2_sum = 0.0\n",
    "        loss3_sum = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            enc_in, dec_in, end_in, enc_true, dec_true, end_true = batch\n",
    "\n",
    "            enc_in   = enc_in.to(device)\n",
    "            dec_in   = dec_in.to(device)\n",
    "            end_in   = end_in.to(device)\n",
    "            enc_true = enc_true.to(device)\n",
    "            dec_true = dec_true.to(device)\n",
    "            end_true = end_true.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            enc_pred, dec_pred, end_pred = model(enc_in, dec_in, end_in)\n",
    "\n",
    "            loss1 = reduce_loss(parameter_loss(enc_true, enc_pred), \"mean\")\n",
    "            loss2 = reduce_loss(albedo_loss(dec_true, dec_pred), \"mean\")\n",
    "            loss3 = reduce_loss(end_to_end_loss(end_true, end_pred), \"mean\")\n",
    "\n",
    "            total_loss = loss_weights[0]*loss1 + loss_weights[1]*loss2 + loss_weights[2]*loss3\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss_sum += total_loss.item()\n",
    "            loss1_sum += loss1.item()\n",
    "            loss2_sum += loss2.item()\n",
    "            loss3_sum += loss3.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        train_stats = {\n",
    "            \"total\": total_loss_sum / max(n_batches, 1),\n",
    "            \"loss1\": loss1_sum / max(n_batches, 1),\n",
    "            \"loss2\": loss2_sum / max(n_batches, 1),\n",
    "            \"loss3\": loss3_sum / max(n_batches, 1),\n",
    "        }\n",
    "\n",
    "        val_stats = evaluate(model, val_loader, loss_weights=loss_weights)\n",
    "\n",
    "        # Keras ReduceLROnPlateau monitors train loss in your code\n",
    "        scheduler.step(train_stats[\"total\"])\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        # Logging record for this epoch\n",
    "        epoch_record = {\n",
    "            \"epoch\": epoch,\n",
    "            \"lr\": current_lr,\n",
    "            \"train\": train_stats,\n",
    "            \"val\": val_stats,\n",
    "        }\n",
    "        history[\"epochs\"].append(epoch_record)\n",
    "\n",
    "        # Save JSON file continuously (optional)\n",
    "        if save_json_each_epoch:\n",
    "            with open(json_path, \"w\") as f:\n",
    "                json.dump(history, f, indent=6)\n",
    "\n",
    "       \n",
    "        # Print callback every N epochs\n",
    "        # Print callback every N epochs\n",
    "        if epoch % print_period == 0:\n",
    "            print(\n",
    "                f\"epoch: {epoch} | \"\n",
    "                f\"train_total={train_stats['total']:.6f} \"\n",
    "                f\"(L1={train_stats['loss2']:.6f}, L2={train_stats['loss1']:.6f}, end={train_stats['loss3']:.6f}) | \"\n",
    "                f\"val_total={val_stats['total']:.6f} \"\n",
    "                f\"(L1={val_stats['loss2']:.6f}, L2={val_stats['loss1']:.6f}, end={val_stats['loss3']:.6f}) | \"\n",
    "                f\"lr={current_lr:.2e}\"\n",
    "            )\n",
    "\n",
    "        # VISUALIZE EVERY EPOCH (not just print_period)\n",
    "        for val_batch in val_loader:\n",
    "            break\n",
    "        end_in, end_true = val_batch[2], val_batch[3]\n",
    "\n",
    "        vis_metrics = show_training_progress(\n",
    "            model.encoder, \n",
    "            model.decoder, \n",
    "            end_in.to(device), \n",
    "            end_true.to(device), \n",
    "            device, \n",
    "            epoch,\n",
    "            viz_dir\n",
    "        )\n",
    "\n",
    "        # LOG TO TEXT FILE EVERY EPOCH\n",
    "        with open(log_file_path, 'a') as f:\n",
    "            f.write(f\"\\n{'='*80}\\n\")\n",
    "            f.write(f\"EPOCH {epoch}\\n\")\n",
    "            f.write(f\"{'='*80}\\n\")\n",
    "            f.write(f\"Train Loss:  {train_stats['total']:.6f} \")\n",
    "            f.write(f\"(param={train_stats['loss1']:.6f}, albedo={train_stats['loss2']:.6f}, e2e={train_stats['loss3']:.6f})\\n\")\n",
    "            f.write(f\"Val Loss:    {val_stats['total']:.6f} \")\n",
    "            f.write(f\"(param={val_stats['loss1']:.6f}, albedo={val_stats['loss2']:.6f}, e2e={val_stats['loss3']:.6f})\\n\")\n",
    "            f.write(f\"LR:          {current_lr:.2e}\\n\")\n",
    "            f.write(f\"Viz Errors:  \")\n",
    "            for m in vis_metrics:\n",
    "                f.write(f\"Sample{m['sample']}={m['mean_error']:.2f} \")\n",
    "            f.write(f\"\\nImage saved: epoch_{epoch:04d}.png\\n\")\n",
    "\n",
    "\n",
    "        # Save BEST model (like ModelCheckpoint(save_best_only=True))\n",
    "        if train_stats[\"total\"] < best_train_loss:\n",
    "            best_train_loss = train_stats[\"total\"]\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"best_train_loss\": best_train_loss,\n",
    "                    \"history_path\": json_path,\n",
    "                },\n",
    "                best_ckpt_path,\n",
    "            )\n",
    "\n",
    "        # Save periodic checkpoint (every checkpoint_period epochs)\n",
    "        if (epoch + 1) % checkpoint_period == 0:\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"train_total_loss\": train_stats[\"total\"],\n",
    "                    \"val_total_loss\": val_stats[\"total\"],\n",
    "                    \"history_path\": json_path,\n",
    "                },\n",
    "                last_ckpt_path,\n",
    "            )\n",
    "\n",
    "    # final save JSON (ensures up-to-date)\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(history, f, indent=6)\n",
    "\n",
    "   # FINAL LOG ENTRY\n",
    "    with open(log_file_path, 'a') as f:\n",
    "        f.write(f\"\\n{'='*80}\\n\")\n",
    "        f.write(\"TRAINING COMPLETE\\n\")\n",
    "        f.write(f\"{'='*80}\\n\")\n",
    "        f.write(f\"End Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Best Training Loss: {best_train_loss:.6f}\\n\")\n",
    "        f.write(f\"Total Visualizations: {num_epochs}\\n\")\n",
    "\n",
    "    print(\"\\nTraining complete!\")\n",
    "    print(f\"All {num_epochs} visualizations saved in: {viz_dir}\")\n",
    "    print(f\"Training log: {log_file_path}\")\n",
    "\n",
    "    return history, run_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2895275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Run folder: checkpoints\\2026-02-24_14-21-28\n",
      "Using device: cuda\n",
      "Run folder: checkpoints\\2026-02-24_14-21-28\n",
      "Visualizations folder: checkpoints\\2026-02-24_14-21-28\\visualizations\n",
      "Training log: checkpoints\\2026-02-24_14-21-28\\visualizations\\training_log.txt\n",
      "epoch: 0 | train_total=1.178348 (L1=1.272290, L2=0.963346, end=1.270191) | val_total=1.149902 (L1=1.240290, L2=0.941673, end=1.238951) | lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.09411764705882353..0.03137254901960784].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.09411764705882353..0.03137254901960784].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.09411764705882353..0.03137254901960784].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0784313725490196..0.047058823529411764].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0784313725490196..0.047058823529411764].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0784313725490196..0.047058823529411764].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.058823529411764705..0.06666666666666667].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.058823529411764705..0.06666666666666667].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.058823529411764705..0.06666666666666667].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0392156862745098..0.08627450980392157].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0392156862745098..0.08235294117647059].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0392156862745098..0.08235294117647059].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.023529411764705882..0.10588235294117647].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.023529411764705882..0.10196078431372549].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.023529411764705882..0.10196078431372549].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | train_total=0.923084 (L1=0.990132, L2=0.751452, end=0.997726) | val_total=0.895173 (L1=0.959515, L2=0.725601, end=0.969235) | lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.00784313725490196..0.12549019607843137].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.00784313725490196..0.12549019607843137].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.00784313725490196..0.12549019607843137].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 | train_total=0.669945 (L1=0.721247, L2=0.496738, end=0.747999) | val_total=0.643182 (L1=0.693925, L2=0.476037, end=0.718298) | lr=1.00e-04\n",
      "epoch: 15 | train_total=0.464970 (L1=0.520159, L2=0.416145, end=0.480183) | val_total=0.448880 (L1=0.505533, L2=0.418347, end=0.454704) | lr=1.00e-04\n",
      "epoch: 20 | train_total=0.374138 (L1=0.451093, L2=0.439661, end=0.328552) | val_total=0.369576 (L1=0.448943, L2=0.439386, end=0.321443) | lr=1.00e-04\n",
      "epoch: 25 | train_total=0.337544 (L1=0.445077, L2=0.445544, end=0.265621) | val_total=0.333309 (L1=0.444931, L2=0.447077, end=0.257822) | lr=1.00e-04\n",
      "epoch: 30 | train_total=0.285490 (L1=0.444067, L2=0.471774, end=0.165918) | val_total=0.278843 (L1=0.442768, L2=0.474941, end=0.153473) | lr=1.00e-04\n",
      "epoch: 35 | train_total=0.240667 (L1=0.428045, L2=0.487443, end=0.086049) | val_total=0.237684 (L1=0.425320, L2=0.485392, end=0.082558) | lr=1.00e-04\n",
      "epoch: 40 | train_total=0.224147 (L1=0.407079, L2=0.462510, end=0.074476) | val_total=0.222786 (L1=0.403691, L2=0.460129, end=0.073964) | lr=1.00e-04\n",
      "epoch: 45 | train_total=0.213741 (L1=0.375449, L2=0.443472, end=0.071924) | val_total=0.212558 (L1=0.371294, L2=0.441698, end=0.071531) | lr=1.00e-04\n",
      "epoch: 50 | train_total=0.204754 (L1=0.342992, L2=0.427892, end=0.070144) | val_total=0.203733 (L1=0.339417, L2=0.426372, end=0.069799) | lr=1.00e-04\n",
      "epoch: 55 | train_total=0.197263 (L1=0.317398, L2=0.414746, end=0.068499) | val_total=0.196388 (L1=0.314706, L2=0.413501, end=0.068113) | lr=1.00e-04\n",
      "epoch: 60 | train_total=0.190351 (L1=0.297330, L2=0.403578, end=0.065908) | val_total=0.189462 (L1=0.294960, L2=0.402444, end=0.065387) | lr=1.00e-04\n",
      "epoch: 65 | train_total=0.183735 (L1=0.280511, L2=0.393892, end=0.062527) | val_total=0.182948 (L1=0.278366, L2=0.392997, end=0.062020) | lr=1.00e-04\n",
      "epoch: 70 | train_total=0.177683 (L1=0.264402, L2=0.385360, end=0.059392) | val_total=0.176886 (L1=0.262028, L2=0.384498, end=0.058890) | lr=1.00e-04\n",
      "epoch: 75 | train_total=0.171418 (L1=0.247617, L2=0.377897, end=0.055478) | val_total=0.170647 (L1=0.245600, L2=0.377276, end=0.054840) | lr=1.00e-04\n",
      "epoch: 80 | train_total=0.164849 (L1=0.233431, L2=0.372311, end=0.049687) | val_total=0.164016 (L1=0.231676, L2=0.371900, end=0.048798) | lr=1.00e-04\n",
      "epoch: 85 | train_total=0.158441 (L1=0.223930, L2=0.368031, end=0.042731) | val_total=0.157763 (L1=0.222796, L2=0.367768, end=0.041922) | lr=1.00e-04\n",
      "epoch: 90 | train_total=0.153358 (L1=0.218745, L2=0.364781, end=0.036749) | val_total=0.152816 (L1=0.217897, L2=0.364611, end=0.036072) | lr=1.00e-04\n",
      "epoch: 95 | train_total=0.149785 (L1=0.214323, L2=0.362369, end=0.032737) | val_total=0.149381 (L1=0.213420, L2=0.362319, end=0.032240) | lr=1.00e-04\n",
      "epoch: 100 | train_total=0.147541 (L1=0.208899, L2=0.360676, end=0.030747) | val_total=0.147304 (L1=0.207869, L2=0.360630, end=0.030547) | lr=1.00e-04\n",
      "epoch: 105 | train_total=0.145947 (L1=0.203128, L2=0.359286, end=0.029748) | val_total=0.145694 (L1=0.202160, L2=0.359281, end=0.029489) | lr=1.00e-04\n",
      "epoch: 110 | train_total=0.144608 (L1=0.197879, L2=0.358109, end=0.028979) | val_total=0.144369 (L1=0.196944, L2=0.358141, end=0.028720) | lr=1.00e-04\n",
      "epoch: 115 | train_total=0.143436 (L1=0.193097, L2=0.357125, end=0.028315) | val_total=0.143271 (L1=0.192272, L2=0.357171, end=0.028154) | lr=1.00e-04\n",
      "epoch: 120 | train_total=0.142370 (L1=0.188816, L2=0.356267, end=0.027680) | val_total=0.142234 (L1=0.187987, L2=0.356299, end=0.027576) | lr=1.00e-04\n",
      "epoch: 125 | train_total=0.141411 (L1=0.184898, L2=0.355460, end=0.027139) | val_total=0.141253 (L1=0.184103, L2=0.355529, end=0.026973) | lr=1.00e-04\n",
      "epoch: 130 | train_total=0.140467 (L1=0.181225, L2=0.354725, end=0.026545) | val_total=0.140439 (L1=0.180503, L2=0.354859, end=0.026552) | lr=1.00e-04\n",
      "epoch: 135 | train_total=0.139618 (L1=0.177876, L2=0.354105, end=0.025999) | val_total=0.139466 (L1=0.177130, L2=0.354208, end=0.025818) | lr=1.00e-04\n",
      "epoch: 140 | train_total=0.138855 (L1=0.174721, L2=0.353522, end=0.025544) | val_total=0.138769 (L1=0.174049, L2=0.353630, end=0.025458) | lr=1.00e-04\n",
      "epoch: 145 | train_total=0.138089 (L1=0.171831, L2=0.353007, end=0.025007) | val_total=0.138076 (L1=0.171192, L2=0.353094, end=0.025047) | lr=1.00e-04\n",
      "epoch: 150 | train_total=0.137417 (L1=0.169191, L2=0.352453, end=0.024604) | val_total=0.137217 (L1=0.168530, L2=0.352563, end=0.024325) | lr=1.00e-04\n",
      "epoch: 155 | train_total=0.136772 (L1=0.166696, L2=0.352011, end=0.024165) | val_total=0.136665 (L1=0.166032, L2=0.352108, end=0.024048) | lr=1.00e-04\n",
      "epoch: 160 | train_total=0.136186 (L1=0.164260, L2=0.351587, end=0.023807) | val_total=0.136052 (L1=0.163677, L2=0.351704, end=0.023621) | lr=1.00e-04\n",
      "epoch: 165 | train_total=0.135448 (L1=0.161746, L2=0.351173, end=0.023202) | val_total=0.135330 (L1=0.161139, L2=0.351309, end=0.023040) | lr=1.00e-04\n",
      "epoch: 170 | train_total=0.134696 (L1=0.159007, L2=0.350804, end=0.022590) | val_total=0.134761 (L1=0.158359, L2=0.350962, end=0.022728) | lr=1.00e-04\n",
      "epoch: 175 | train_total=0.134032 (L1=0.156026, L2=0.350465, end=0.022150) | val_total=0.133913 (L1=0.155397, L2=0.350588, end=0.021994) | lr=1.00e-04\n",
      "epoch: 180 | train_total=0.133522 (L1=0.153754, L2=0.350095, end=0.021863) | val_total=0.133573 (L1=0.153196, L2=0.350273, end=0.021953) | lr=1.00e-04\n",
      "epoch: 185 | train_total=0.133066 (L1=0.151789, L2=0.349786, end=0.021585) | val_total=0.133010 (L1=0.151225, L2=0.349954, end=0.021502) | lr=1.00e-04\n",
      "epoch: 190 | train_total=0.132637 (L1=0.150006, L2=0.349542, end=0.021290) | val_total=0.132542 (L1=0.149453, L2=0.349654, end=0.021168) | lr=1.00e-04\n",
      "epoch: 195 | train_total=0.132230 (L1=0.148340, L2=0.349192, end=0.021064) | val_total=0.132271 (L1=0.147788, L2=0.349375, end=0.021132) | lr=1.00e-04\n",
      "\n",
      "Training complete!\n",
      "All 200 visualizations saved in: checkpoints\\2026-02-24_14-21-28\\visualizations\n",
      "Training log: checkpoints\\2026-02-24_14-21-28\\visualizations\\training_log.txt\n",
      "Best checkpoint saved at: checkpoints\\2026-02-24_14-21-28\\best.pt\n",
      "Run directory: checkpoints\\2026-02-24_14-21-28\n"
     ]
    }
   ],
   "source": [
    "encoder_net = Encoder().to(device)\n",
    "decoder_net = Decoder().to(device)\n",
    "model = AutoEncoder(encoder_net, decoder_net).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",\n",
    "    factor=0.01,\n",
    "    patience=5,\n",
    "    threshold=1e-4,\n",
    "    cooldown=0,\n",
    "    min_lr=MLR,\n",
    ")\n",
    "\n",
    "history, run_dir = train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    loss_weights=(0.3, 0.1, 0.6),\n",
    "    base_ckpt_dir=\"checkpoints\",\n",
    "    checkpoint_period=200,\n",
    "    print_period=5,\n",
    "    save_json_each_epoch=True,\n",
    ")\n",
    "\n",
    "best_checkpoint = os.path.join(run_dir, \"best.pt\")\n",
    "print(\"Best checkpoint saved at:\", best_checkpoint)\n",
    "print(\"Run directory:\", run_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9bf566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import math\n",
    "\n",
    "# device setup (equivalent to tf.device('/device:GPU:0'))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Make sure encoder / decoder are already defined and moved to device:\n",
    "# encoder.to(device)\n",
    "# decoder.to(device)\n",
    "# encoder.eval()\n",
    "# decoder.eval()\n",
    "\n",
    "\n",
    "def encode(image, encoder):\n",
    "    \"\"\"\n",
    "    PyTorch version of TensorFlow encode()\n",
    "    Input: image (numpy array), encoder (torch.nn.Module)\n",
    "    Output: pred_maps (numpy), elapsed (float), (WIDTH, HEIGHT)\n",
    "    \"\"\"\n",
    "    print(f\"Image shape at start of encoder method: {image.shape}\")\n",
    "\n",
    "    # Handle 2D flattened input vs image input\n",
    "    if len(image.shape) == 2:\n",
    "        WIDTH = HEIGHT = int(math.sqrt(image.shape[0]))\n",
    "        # your code: reshape(-1,4) then later reshape(W*H,3)\n",
    "        # This implies the 4th channel was ignored later.\n",
    "        # We'll keep it identical:\n",
    "        image = np.asarray(image).reshape(-1, 4).astype(\"float32\")\n",
    "    else:\n",
    "        WIDTH = image.shape[0]\n",
    "        HEIGHT = image.shape[1]\n",
    "        image = np.asarray(image).astype(\"float32\")\n",
    "\n",
    "    # match your Keras reshape\n",
    "    image = image.reshape(WIDTH * HEIGHT, 3)\n",
    "\n",
    "    start = time.time()\n",
    "    print(f\"Image shape before encoder inference: {image.shape}\")\n",
    "\n",
    "    # Convert to torch tensor and run model inference\n",
    "    x = torch.from_numpy(image).to(device)\n",
    "\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_maps = encoder(x)\n",
    "\n",
    "    # Convert back to numpy\n",
    "    pred_maps = pred_maps.detach().cpu().numpy()\n",
    "\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "\n",
    "    # reshape output to (WIDTH*HEIGHT, 5)\n",
    "    pred_maps = pred_maps.reshape(WIDTH * HEIGHT, 5)\n",
    "\n",
    "    return pred_maps, elapsed, (WIDTH, HEIGHT)\n",
    "\n",
    "\n",
    "def decode(encoded, decoder):\n",
    "    \"\"\"\n",
    "    PyTorch version of TensorFlow decode()\n",
    "    Input: encoded (numpy array), decoder (torch.nn.Module)\n",
    "    Output: recovered (numpy), elapsed (float), (WIDTH, HEIGHT)\n",
    "    \"\"\"\n",
    "    print(f\"Image shape going into encoder: {encoded.shape}\")\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Handle 2D flattened input vs (W,H,C) style input\n",
    "    if len(encoded.shape) == 2:\n",
    "        WIDTH = HEIGHT = int(math.sqrt(encoded.shape[0]))\n",
    "        encoded = np.asarray(encoded).reshape(-1, 5).astype(\"float32\")\n",
    "    else:\n",
    "        WIDTH = encoded.shape[0]\n",
    "        HEIGHT = encoded.shape[1]\n",
    "        encoded = np.asarray(encoded).astype(\"float32\")\n",
    "\n",
    "    print(f\"encoded shape going into decoder: {encoded.shape}\")\n",
    "\n",
    "    # Torch inference\n",
    "    x = torch.from_numpy(encoded).to(device)\n",
    "\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        recovered = decoder(x)\n",
    "\n",
    "    recovered = recovered.detach().cpu().numpy()\n",
    "\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "\n",
    "    # reshape output to RGB image\n",
    "    recovered = recovered.reshape(WIDTH, HEIGHT, 3)\n",
    "\n",
    "    return recovered, elapsed, (WIDTH, HEIGHT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b26b00",
   "metadata": {},
   "source": [
    "# AE with Clipping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea957648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import json\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# DEVICE\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# MODEL COMPONENTS\n",
    "# -----------------------------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim=3, hidden_dim=NUM_NEURONS, num_layers=NUM_LAYERS, out_dim=5):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Linear(in_dim if i == 0 else hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.out = nn.Linear(hidden_dim, out_dim)\n",
    "        \n",
    "        self.register_buffer('param_mins', torch.tensor([0.001, 0.001, 0.0, 0.6, 0.01]))\n",
    "        self.register_buffer('param_maxs', torch.tensor([0.5, 0.311, 1.0, 0.98, 0.25]))           \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        # Output layer with clamping\n",
    "        params = self.out(x)\n",
    "        \n",
    "        # Clamp to valid parameter ranges (CRITICAL!)\n",
    "        params_clamped = torch.zeros_like(params)\n",
    "        for i in range(5):\n",
    "            params_clamped[:, i] = torch.clamp(\n",
    "                params[:, i], \n",
    "                self.param_mins[i], \n",
    "                self.param_maxs[i]\n",
    "            )\n",
    "        \n",
    "        return params_clamped\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_dim=5, hidden_dim=NUM_NEURONS, num_layers=NUM_LAYERS, out_dim=3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Linear(in_dim if i == 0 else hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.out = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        # Output layer (linear, no activation)\n",
    "        rgb = self.out(x)\n",
    "        \n",
    "        # Clamp RGB to [0, 1] range\n",
    "        rgb = torch.clamp(rgb, 0.0, 1.0)\n",
    "        \n",
    "        return rgb\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, encoder: nn.Module, decoder: nn.Module):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, encoder_in, decoder_in, end_to_end_in):\n",
    "        enc_out = self.encoder(encoder_in)\n",
    "        dec_out = self.decoder(decoder_in)\n",
    "        end_out = self.decoder(self.encoder(end_to_end_in))\n",
    "        return enc_out, dec_out, end_out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Loss functions (same as before)\n",
    "# -----------------------------\n",
    "def albedo_loss(y_true, y_pred):\n",
    "    return torch.sum(torch.abs(y_pred - y_true), dim=-1)\n",
    "\n",
    "def parameter_loss(y_true, y_pred):\n",
    "    return torch.sqrt(torch.sum((y_pred - y_true) ** 2, dim=-1) + 1e-12)\n",
    "\n",
    "def end_to_end_loss(y_true, y_pred):\n",
    "    return torch.sum(torch.abs(y_pred - y_true), dim=-1)\n",
    "\n",
    "def reduce_loss(loss_per_sample, reduction=\"mean\"):\n",
    "    if reduction == \"mean\":\n",
    "        return loss_per_sample.mean()\n",
    "    elif reduction == \"sum\":\n",
    "        return loss_per_sample.sum()\n",
    "    else:\n",
    "        raise ValueError(\"reduction must be 'mean' or 'sum'\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Run folder creation (date/time)\n",
    "# -----------------------------\n",
    "def create_run_folder(base_dir=\"checkpoints\"):\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    run_dir = os.path.join(base_dir, now)\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    return run_dir\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation loop (val)\n",
    "# -----------------------------\n",
    "def evaluate(model, dataloader, loss_weights=(0.3, 0.1, 0.6)):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss_sum = 0.0\n",
    "    loss1_sum = 0.0\n",
    "    loss2_sum = 0.0\n",
    "    loss3_sum = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            enc_in, dec_in, end_in, enc_true, dec_true, end_true = batch\n",
    "\n",
    "            enc_in   = enc_in.to(device)\n",
    "            dec_in   = dec_in.to(device)\n",
    "            end_in   = end_in.to(device)\n",
    "            enc_true = enc_true.to(device)\n",
    "            dec_true = dec_true.to(device)\n",
    "            end_true = end_true.to(device)\n",
    "\n",
    "            enc_pred, dec_pred, end_pred = model(enc_in, dec_in, end_in)\n",
    "\n",
    "            loss1 = reduce_loss(parameter_loss(enc_true, enc_pred), \"mean\")\n",
    "            loss2 = reduce_loss(albedo_loss(dec_true, dec_pred), \"mean\")\n",
    "            loss3 = reduce_loss(end_to_end_loss(end_true, end_pred), \"mean\")\n",
    "\n",
    "            total_loss = loss_weights[0]*loss1 + loss_weights[1]*loss2 + loss_weights[2]*loss3\n",
    "\n",
    "            total_loss_sum += total_loss.item()\n",
    "            loss1_sum += loss1.item()\n",
    "            loss2_sum += loss2.item()\n",
    "            loss3_sum += loss3.item()\n",
    "            n_batches += 1\n",
    "\n",
    "    if n_batches == 0:\n",
    "        return {\"total\": 0.0, \"loss1\": 0.0, \"loss2\": 0.0, \"loss3\": 0.0}\n",
    "\n",
    "    return {\n",
    "        \"total\": total_loss_sum / n_batches,\n",
    "        \"loss1\": loss1_sum / n_batches,\n",
    "        \"loss2\": loss2_sum / n_batches,\n",
    "        \"loss3\": loss3_sum / n_batches,\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training loop (train + val)\n",
    "# -----------------------------\n",
    "def train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    num_epochs=200,\n",
    "    loss_weights=(0.3, 0.1, 0.6),\n",
    "    base_ckpt_dir=\"checkpoints\",\n",
    "    checkpoint_period=200,\n",
    "    print_period=25,\n",
    "    save_json_each_epoch=True,\n",
    "):\n",
    "    run_dir = create_run_folder(base_ckpt_dir)\n",
    "    best_ckpt_path = os.path.join(run_dir, \"best.pt\")\n",
    "    last_ckpt_path = os.path.join(run_dir, \"last.pt\")\n",
    "    json_path = os.path.join(run_dir, \"history.json\")\n",
    "\n",
    "    print(\"Using device:\", device)\n",
    "    print(\"Run folder:\", run_dir)\n",
    "\n",
    "    # CREATE VISUALIZATION FOLDER\n",
    "    viz_dir = os.path.join(run_dir, \"visualizations\")\n",
    "    os.makedirs(viz_dir, exist_ok=True)\n",
    "    \n",
    "    # CREATE LOG FILE\n",
    "    log_file_path = os.path.join(viz_dir, \"training_log.txt\")\n",
    "    with open(log_file_path, 'w') as f:\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"AUTOENCODER TRAINING LOG\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(f\"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Device: {device}\\n\")\n",
    "        f.write(f\"Number of Epochs: {num_epochs}\\n\")\n",
    "        f.write(f\"Batch Size: {train_loader.batch_size}\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "    print(\"Using device:\", device)\n",
    "    print(\"Run folder:\", run_dir)\n",
    "    print(\"Visualizations folder:\", viz_dir)\n",
    "    print(\"Training log:\", log_file_path)\n",
    "\n",
    "    best_train_loss = float(\"inf\")\n",
    "\n",
    "    history = {\n",
    "        \"config\": {\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"loss_weights\": loss_weights,\n",
    "            \"optimizer\": optimizer.__class__.__name__,\n",
    "            \"scheduler\": scheduler.__class__.__name__,\n",
    "            \"device\": str(device),\n",
    "        },\n",
    "        \"epochs\": []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        total_loss_sum = 0.0\n",
    "        loss1_sum = 0.0\n",
    "        loss2_sum = 0.0\n",
    "        loss3_sum = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            enc_in, dec_in, end_in, enc_true, dec_true, end_true = batch\n",
    "\n",
    "            enc_in   = enc_in.to(device)\n",
    "            dec_in   = dec_in.to(device)\n",
    "            end_in   = end_in.to(device)\n",
    "            enc_true = enc_true.to(device)\n",
    "            dec_true = dec_true.to(device)\n",
    "            end_true = end_true.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            enc_pred, dec_pred, end_pred = model(enc_in, dec_in, end_in)\n",
    "\n",
    "            loss1 = reduce_loss(parameter_loss(enc_true, enc_pred), \"mean\")\n",
    "            loss2 = reduce_loss(albedo_loss(dec_true, dec_pred), \"mean\")\n",
    "            loss3 = reduce_loss(end_to_end_loss(end_true, end_pred), \"mean\")\n",
    "\n",
    "            total_loss = loss_weights[0]*loss1 + loss_weights[1]*loss2 + loss_weights[2]*loss3\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss_sum += total_loss.item()\n",
    "            loss1_sum += loss1.item()\n",
    "            loss2_sum += loss2.item()\n",
    "            loss3_sum += loss3.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        train_stats = {\n",
    "            \"total\": total_loss_sum / max(n_batches, 1),\n",
    "            \"loss1\": loss1_sum / max(n_batches, 1),\n",
    "            \"loss2\": loss2_sum / max(n_batches, 1),\n",
    "            \"loss3\": loss3_sum / max(n_batches, 1),\n",
    "        }\n",
    "\n",
    "        val_stats = evaluate(model, val_loader, loss_weights=loss_weights)\n",
    "\n",
    "        # Keras ReduceLROnPlateau monitors train loss in your code\n",
    "        scheduler.step(train_stats[\"total\"])\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        # Logging record for this epoch\n",
    "        epoch_record = {\n",
    "            \"epoch\": epoch,\n",
    "            \"lr\": current_lr,\n",
    "            \"train\": train_stats,\n",
    "            \"val\": val_stats,\n",
    "        }\n",
    "        history[\"epochs\"].append(epoch_record)\n",
    "\n",
    "        # Save JSON file continuously (optional)\n",
    "        if save_json_each_epoch:\n",
    "            with open(json_path, \"w\") as f:\n",
    "                json.dump(history, f, indent=6)\n",
    "\n",
    "       \n",
    "        # Print callback every N epochs\n",
    "        # Print callback every N epochs\n",
    "        if epoch % print_period == 0:\n",
    "            print(\n",
    "                f\"epoch: {epoch} | \"\n",
    "                f\"train_total={train_stats['total']:.6f} \"\n",
    "                f\"(L1={train_stats['loss2']:.6f}, L2={train_stats['loss1']:.6f}, end={train_stats['loss3']:.6f}) | \"\n",
    "                f\"val_total={val_stats['total']:.6f} \"\n",
    "                f\"(L1={val_stats['loss2']:.6f}, L2={val_stats['loss1']:.6f}, end={val_stats['loss3']:.6f}) | \"\n",
    "                f\"lr={current_lr:.2e}\"\n",
    "            )\n",
    "\n",
    "        # VISUALIZE EVERY EPOCH (not just print_period)\n",
    "        for val_batch in val_loader:\n",
    "            break\n",
    "        end_in, end_true = val_batch[2], val_batch[3]\n",
    "\n",
    "        vis_metrics = show_training_progress(\n",
    "            model.encoder, \n",
    "            model.decoder, \n",
    "            end_in.to(device), \n",
    "            end_true.to(device), \n",
    "            device, \n",
    "            epoch,\n",
    "            viz_dir\n",
    "        )\n",
    "\n",
    "        # LOG TO TEXT FILE EVERY EPOCH\n",
    "        with open(log_file_path, 'a') as f:\n",
    "            f.write(f\"\\n{'='*80}\\n\")\n",
    "            f.write(f\"EPOCH {epoch}\\n\")\n",
    "            f.write(f\"{'='*80}\\n\")\n",
    "            f.write(f\"Train Loss:  {train_stats['total']:.6f} \")\n",
    "            f.write(f\"(param={train_stats['loss1']:.6f}, albedo={train_stats['loss2']:.6f}, e2e={train_stats['loss3']:.6f})\\n\")\n",
    "            f.write(f\"Val Loss:    {val_stats['total']:.6f} \")\n",
    "            f.write(f\"(param={val_stats['loss1']:.6f}, albedo={val_stats['loss2']:.6f}, e2e={val_stats['loss3']:.6f})\\n\")\n",
    "            f.write(f\"LR:          {current_lr:.2e}\\n\")\n",
    "            f.write(f\"Viz Errors:  \")\n",
    "            for m in vis_metrics:\n",
    "                f.write(f\"Sample{m['sample']}={m['mean_error']:.2f} \")\n",
    "            f.write(f\"\\nImage saved: epoch_{epoch:04d}.png\\n\")\n",
    "\n",
    "\n",
    "        # Save BEST model (like ModelCheckpoint(save_best_only=True))\n",
    "        if train_stats[\"total\"] < best_train_loss:\n",
    "            best_train_loss = train_stats[\"total\"]\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"best_train_loss\": best_train_loss,\n",
    "                    \"history_path\": json_path,\n",
    "                },\n",
    "                best_ckpt_path,\n",
    "            )\n",
    "\n",
    "        # Save periodic checkpoint (every checkpoint_period epochs)\n",
    "        if (epoch + 1) % checkpoint_period == 0:\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"train_total_loss\": train_stats[\"total\"],\n",
    "                    \"val_total_loss\": val_stats[\"total\"],\n",
    "                    \"history_path\": json_path,\n",
    "                },\n",
    "                last_ckpt_path,\n",
    "            )\n",
    "\n",
    "    # final save JSON (ensures up-to-date)\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(history, f, indent=6)\n",
    "\n",
    "   # FINAL LOG ENTRY\n",
    "    with open(log_file_path, 'a') as f:\n",
    "        f.write(f\"\\n{'='*80}\\n\")\n",
    "        f.write(\"TRAINING COMPLETE\\n\")\n",
    "        f.write(f\"{'='*80}\\n\")\n",
    "        f.write(f\"End Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Best Training Loss: {best_train_loss:.6f}\\n\")\n",
    "        f.write(f\"Total Visualizations: {num_epochs}\\n\")\n",
    "\n",
    "    print(\"\\nTraining complete!\")\n",
    "    print(f\"All {num_epochs} visualizations saved in: {viz_dir}\")\n",
    "    print(f\"Training log: {log_file_path}\")\n",
    "\n",
    "    return history, run_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a0e32c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Run folder: checkpoints/AE\\2026-02-26_19-13-01\n",
      "Using device: cuda\n",
      "Run folder: checkpoints/AE\\2026-02-26_19-13-01\n",
      "Visualizations folder: checkpoints/AE\\2026-02-26_19-13-01\\visualizations\n",
      "Training log: checkpoints/AE\\2026-02-26_19-13-01\\visualizations\\training_log.txt\n",
      "epoch: 0 | train_total=1.141988 (L1=1.093705, L2=0.525100, end=1.093911) | val_total=1.102654 (L1=1.054695, L2=0.510436, end=1.055191) | lr=1.00e-04\n",
      "epoch: 5 | train_total=0.669793 (L1=0.594028, L2=0.456020, end=0.591298) | val_total=0.666426 (L1=0.591222, L2=0.455295, end=0.587452) | lr=1.00e-04\n",
      "epoch: 10 | train_total=0.653435 (L1=0.576106, L2=0.440972, end=0.580519) | val_total=0.645496 (L1=0.572048, L2=0.434327, end=0.572639) | lr=1.00e-04\n",
      "epoch: 15 | train_total=0.495555 (L1=0.481129, L2=0.434492, end=0.368114) | val_total=0.489078 (L1=0.471503, L2=0.433614, end=0.362571) | lr=1.00e-04\n",
      "epoch: 20 | train_total=0.466735 (L1=0.426003, L2=0.429465, end=0.350157) | val_total=0.465040 (L1=0.423053, L2=0.429512, end=0.348785) | lr=1.00e-04\n",
      "epoch: 25 | train_total=0.458938 (L1=0.414183, L2=0.427743, end=0.343933) | val_total=0.458240 (L1=0.412843, L2=0.427816, end=0.343405) | lr=1.00e-04\n",
      "epoch: 30 | train_total=0.455529 (L1=0.406662, L2=0.426882, end=0.342444) | val_total=0.454670 (L1=0.405157, L2=0.427175, end=0.341617) | lr=1.00e-04\n",
      "epoch: 35 | train_total=0.451937 (L1=0.396806, L2=0.426338, end=0.341656) | val_total=0.450792 (L1=0.395103, L2=0.426567, end=0.340484) | lr=1.00e-04\n",
      "epoch: 40 | train_total=0.449424 (L1=0.387891, L2=0.425957, end=0.342116) | val_total=0.449141 (L1=0.386471, L2=0.426238, end=0.342214) | lr=1.00e-04\n",
      "epoch: 45 | train_total=0.446110 (L1=0.378790, L2=0.425575, end=0.341335) | val_total=0.445339 (L1=0.377323, L2=0.425753, end=0.340694) | lr=1.00e-04\n",
      "epoch: 50 | train_total=0.443678 (L1=0.370122, L2=0.425262, end=0.341771) | val_total=0.442116 (L1=0.368671, L2=0.425463, end=0.339792) | lr=1.00e-04\n",
      "epoch: 55 | train_total=0.441546 (L1=0.362922, L2=0.424949, end=0.341974) | val_total=0.439758 (L1=0.361529, L2=0.425176, end=0.339577) | lr=1.00e-04\n",
      "epoch: 60 | train_total=0.439587 (L1=0.357139, L2=0.424711, end=0.341719) | val_total=0.440302 (L1=0.356791, L2=0.424928, end=0.342977) | lr=1.00e-04\n",
      "epoch: 65 | train_total=0.437878 (L1=0.352707, L2=0.424519, end=0.341183) | val_total=0.438118 (L1=0.352256, L2=0.424714, end=0.341712) | lr=1.00e-04\n",
      "epoch: 70 | train_total=0.437045 (L1=0.349413, L2=0.424287, end=0.341558) | val_total=0.436381 (L1=0.348754, L2=0.424535, end=0.340658) | lr=1.00e-04\n",
      "epoch: 75 | train_total=0.435662 (L1=0.346352, L2=0.424156, end=0.340849) | val_total=0.434842 (L1=0.345567, L2=0.424375, end=0.339765) | lr=1.00e-04\n",
      "epoch: 80 | train_total=0.434713 (L1=0.343885, L2=0.423989, end=0.340584) | val_total=0.433859 (L1=0.343063, L2=0.424216, end=0.339459) | lr=1.00e-04\n",
      "epoch: 85 | train_total=0.434241 (L1=0.341993, L2=0.423828, end=0.340824) | val_total=0.434322 (L1=0.341881, L2=0.424063, end=0.340899) | lr=1.00e-04\n",
      "epoch: 90 | train_total=0.433535 (L1=0.340339, L2=0.423680, end=0.340549) | val_total=0.434133 (L1=0.340477, L2=0.423912, end=0.341360) | lr=1.00e-04\n",
      "epoch: 95 | train_total=0.432994 (L1=0.339026, L2=0.423557, end=0.340365) | val_total=0.432125 (L1=0.338201, L2=0.423751, end=0.339233) | lr=1.00e-04\n",
      "epoch: 100 | train_total=0.431767 (L1=0.337613, L2=0.423354, end=0.339129) | val_total=0.431579 (L1=0.337375, L2=0.423606, end=0.338808) | lr=1.00e-06\n",
      "epoch: 105 | train_total=0.431755 (L1=0.337600, L2=0.423379, end=0.339103) | val_total=0.431563 (L1=0.337351, L2=0.423605, end=0.338794) | lr=1.00e-06\n",
      "epoch: 110 | train_total=0.431726 (L1=0.337578, L2=0.423336, end=0.339087) | val_total=0.431554 (L1=0.337339, L2=0.423600, end=0.338788) | lr=1.00e-06\n",
      "epoch: 115 | train_total=0.431733 (L1=0.337560, L2=0.423381, end=0.339085) | val_total=0.431547 (L1=0.337326, L2=0.423596, end=0.338784) | lr=1.00e-06\n",
      "epoch: 120 | train_total=0.431729 (L1=0.337558, L2=0.423370, end=0.339084) | val_total=0.431541 (L1=0.337316, L2=0.423592, end=0.338780) | lr=1.00e-06\n",
      "epoch: 125 | train_total=0.431706 (L1=0.337544, L2=0.423318, end=0.339079) | val_total=0.431534 (L1=0.337305, L2=0.423588, end=0.338777) | lr=1.00e-06\n",
      "epoch: 130 | train_total=0.431708 (L1=0.337526, L2=0.423366, end=0.339067) | val_total=0.431527 (L1=0.337292, L2=0.423584, end=0.338774) | lr=1.00e-06\n",
      "epoch: 135 | train_total=0.431682 (L1=0.337495, L2=0.423351, end=0.339046) | val_total=0.431520 (L1=0.337278, L2=0.423580, end=0.338772) | lr=1.00e-06\n",
      "epoch: 140 | train_total=0.431700 (L1=0.337515, L2=0.423321, end=0.339081) | val_total=0.431514 (L1=0.337265, L2=0.423576, end=0.338769) | lr=1.00e-06\n",
      "epoch: 145 | train_total=0.431667 (L1=0.337464, L2=0.423346, end=0.339041) | val_total=0.431507 (L1=0.337253, L2=0.423572, end=0.338766) | lr=1.00e-06\n",
      "epoch: 150 | train_total=0.431695 (L1=0.337484, L2=0.423361, end=0.339069) | val_total=0.431500 (L1=0.337238, L2=0.423568, end=0.338763) | lr=1.00e-06\n",
      "epoch: 155 | train_total=0.431620 (L1=0.337410, L2=0.423302, end=0.339011) | val_total=0.431492 (L1=0.337223, L2=0.423563, end=0.338761) | lr=1.00e-06\n",
      "epoch: 160 | train_total=0.431654 (L1=0.337444, L2=0.423289, end=0.339057) | val_total=0.431485 (L1=0.337208, L2=0.423559, end=0.338758) | lr=1.00e-06\n",
      "epoch: 165 | train_total=0.431652 (L1=0.337418, L2=0.423340, end=0.339041) | val_total=0.431477 (L1=0.337194, L2=0.423554, end=0.338754) | lr=1.00e-06\n",
      "epoch: 170 | train_total=0.431656 (L1=0.337422, L2=0.423331, end=0.339051) | val_total=0.431469 (L1=0.337181, L2=0.423550, end=0.338750) | lr=1.00e-06\n",
      "epoch: 175 | train_total=0.431626 (L1=0.337385, L2=0.423303, end=0.339032) | val_total=0.431461 (L1=0.337162, L2=0.423545, end=0.338748) | lr=1.00e-06\n",
      "epoch: 180 | train_total=0.431617 (L1=0.337371, L2=0.423291, end=0.339031) | val_total=0.431453 (L1=0.337148, L2=0.423541, end=0.338744) | lr=1.00e-06\n",
      "epoch: 185 | train_total=0.431610 (L1=0.337358, L2=0.423289, end=0.339027) | val_total=0.431445 (L1=0.337131, L2=0.423537, end=0.338741) | lr=1.00e-06\n",
      "epoch: 190 | train_total=0.431632 (L1=0.337383, L2=0.423253, end=0.339068) | val_total=0.431437 (L1=0.337116, L2=0.423532, end=0.338737) | lr=1.00e-06\n",
      "epoch: 195 | train_total=0.431605 (L1=0.337330, L2=0.423298, end=0.339029) | val_total=0.431428 (L1=0.337098, L2=0.423528, end=0.338734) | lr=1.00e-06\n",
      "\n",
      "Training complete!\n",
      "All 200 visualizations saved in: checkpoints/AE\\2026-02-26_19-13-01\\visualizations\n",
      "Training log: checkpoints/AE\\2026-02-26_19-13-01\\visualizations\\training_log.txt\n",
      "Best checkpoint saved at: checkpoints/AE\\2026-02-26_19-13-01\\best.pt\n",
      "Run directory: checkpoints/AE\\2026-02-26_19-13-01\n"
     ]
    }
   ],
   "source": [
    "encoder_net = Encoder().to(device)\n",
    "decoder_net = Decoder().to(device)\n",
    "model = AutoEncoder(encoder_net, decoder_net).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",\n",
    "    factor=0.01,\n",
    "    patience=5,\n",
    "    threshold=1e-4,\n",
    "    cooldown=0,\n",
    "    min_lr=MLR,\n",
    ")\n",
    "\n",
    "history, run_dir = train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    loss_weights=(0.3, 0.3, 0.6),\n",
    "    base_ckpt_dir=\"checkpoints/AE\",\n",
    "    checkpoint_period=200,\n",
    "    print_period=5,\n",
    "    save_json_each_epoch=True,\n",
    ")\n",
    "\n",
    "best_checkpoint = os.path.join(run_dir, \"best.pt\")\n",
    "print(\"Best checkpoint saved at:\", best_checkpoint)\n",
    "print(\"Run directory:\", run_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4ec3d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb05b49d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac8c2cdc",
   "metadata": {},
   "source": [
    "# SAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9415afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    4-layer encoder with skip connections\n",
    "    Input: RGB (3) -> Output: Parameters (5)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim=3, hidden_dim=75, num_layers=4, out_dim=5):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Input layer\n",
    "        self.input_layer = nn.Linear(in_dim, hidden_dim)\n",
    "        \n",
    "        # Hidden layers\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_dim, hidden_dim) for _ in range(num_layers - 1)\n",
    "        ])\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_dim, out_dim)\n",
    "        \n",
    "        # Parameter ranges for clamping\n",
    "        self.param_mins = torch.tensor([0.001, 0.001, 0.0, 0.6, 0.01])\n",
    "        self.param_maxs = torch.tensor([0.62, 0.32, 1.0, 0.98, 0.25])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input layer\n",
    "        h = torch.relu(self.input_layer(x))\n",
    "        \n",
    "        # Hidden layers with skip connections\n",
    "        previous = h\n",
    "        for i, layer in enumerate(self.hidden_layers):\n",
    "            h = torch.relu(layer(h))\n",
    "            \n",
    "            # Add skip connection from previous layer (every other layer)\n",
    "            if i > 0 and i % 2 == 1:\n",
    "                h = h + previous\n",
    "            \n",
    "            if i % 2 == 0:\n",
    "                previous = h\n",
    "        \n",
    "        # Output layer with clamping\n",
    "        params = self.output_layer(h)\n",
    "        \n",
    "        # Clamp to valid parameter ranges (CRITICAL!)\n",
    "        params_clamped = torch.zeros_like(params)\n",
    "        for i in range(5):\n",
    "            params_clamped[:, i] = torch.clamp(\n",
    "                params[:, i], \n",
    "                self.param_mins[i], \n",
    "                self.param_maxs[i]\n",
    "            )\n",
    "        \n",
    "        return params_clamped\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    4-layer decoder with skip connections\n",
    "    Input: Parameters (5) -> Output: RGB (3)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim=5, hidden_dim=75, num_layers=4, out_dim=3):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Input layer\n",
    "        self.input_layer = nn.Linear(in_dim, hidden_dim)\n",
    "        \n",
    "        # Hidden layers\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_dim, hidden_dim) for _ in range(num_layers - 1)\n",
    "        ])\n",
    "        \n",
    "        # Output layer (no activation - linear output)\n",
    "        self.output_layer = nn.Linear(hidden_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input layer\n",
    "        h = torch.relu(self.input_layer(x))\n",
    "        \n",
    "        # Hidden layers with skip connections\n",
    "        previous = h\n",
    "        for i, layer in enumerate(self.hidden_layers):\n",
    "            h = torch.relu(layer(h))\n",
    "            \n",
    "            # Add skip connection from previous layer (every other layer)\n",
    "            if i > 0 and i % 2 == 1:\n",
    "                h = h + previous\n",
    "            \n",
    "            if i % 2 == 0:\n",
    "                previous = h\n",
    "        \n",
    "        # Output layer (linear, no activation)\n",
    "        rgb = self.output_layer(h)\n",
    "        \n",
    "        # Clamp RGB to [0, 1] range\n",
    "        rgb = torch.clamp(rgb, 0.0, 1.0)\n",
    "        \n",
    "        return rgb\n",
    "\n",
    "\n",
    "class SupervisedAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete SAE with encoder and decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=75, num_layers=4):\n",
    "        super(SupervisedAutoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(\n",
    "            in_dim=3, \n",
    "            hidden_dim=hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            out_dim=5\n",
    "        )\n",
    "        \n",
    "        self.decoder = Decoder(\n",
    "            in_dim=5, \n",
    "            hidden_dim=hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            out_dim=3\n",
    "        )\n",
    "    \n",
    "    def forward(self, rgb):\n",
    "        \"\"\"\n",
    "        Forward pass: RGB -> Parameters -> RGB\n",
    "        \"\"\"\n",
    "        params = self.encoder(rgb)\n",
    "        rgb_reconstructed = self.decoder(params)\n",
    "        \n",
    "        return params, rgb_reconstructed\n",
    "    \n",
    "    def encode(self, rgb):\n",
    "        \"\"\"Encode RGB to parameters\"\"\"\n",
    "        return self.encoder(rgb)\n",
    "    \n",
    "    def decode(self, params):\n",
    "        \"\"\"Decode parameters to RGB\"\"\"\n",
    "        return self.decoder(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67153604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Loss function (from paper Section 3.1.2)\n",
    "class SAELoss(nn.Module):\n",
    "    def __init__(self, alpha=0.3, beta=0.3, gamma=0.6):\n",
    "        \"\"\"\n",
    "        alpha: encoder loss weight (L2 norm)\n",
    "        beta: decoder loss weight (L1 norm)\n",
    "        gamma: end-to-end loss weight (L1 norm)\n",
    "        \"\"\"\n",
    "        super(SAELoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.l2_loss = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, rgb_input, params_pred, params_true, rgb_recon):\n",
    "        \"\"\"\n",
    "        rgb_input: original RGB\n",
    "        params_pred: predicted parameters from encoder\n",
    "        params_true: ground truth parameters\n",
    "        rgb_recon: reconstructed RGB from decoder\n",
    "        \"\"\"\n",
    "        # Encoder loss (L2 norm between predicted and true parameters)\n",
    "        encoder_loss = self.l2_loss(params_pred, params_true)\n",
    "        \n",
    "        # Decoder loss (L1 norm between reconstructed and true RGB)\n",
    "        decoder_loss = self.l1_loss(rgb_recon, rgb_input)\n",
    "        \n",
    "        # End-to-end loss (L1 norm between input and reconstructed RGB)\n",
    "        end_to_end_loss = self.l1_loss(rgb_recon, rgb_input)\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = (\n",
    "            self.alpha * encoder_loss + \n",
    "            self.beta * decoder_loss + \n",
    "            self.gamma * end_to_end_loss\n",
    "        )\n",
    "        \n",
    "        return total_loss, encoder_loss, decoder_loss, end_to_end_loss\n",
    "\n",
    "\n",
    "# Dataset\n",
    "class LUTDataset(Dataset):\n",
    "    def __init__(self, parquet_path):\n",
    "        \"\"\"Load from Parquet (7x faster than CSV!)\"\"\"\n",
    "        self.data = pd.read_parquet(parquet_path)\n",
    "        \n",
    "        # Extract RGB and parameters\n",
    "        self.rgb = self.data[['R', 'G', 'B']].values.astype(np.float32)\n",
    "        self.params = self.data[['Cm', 'Ch', 'Bm', 'Bh', 'T']].values.astype(np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.rgb[idx]),\n",
    "            torch.from_numpy(self.params[idx])\n",
    "        )\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_sae(data_path, epochs=200, batch_size=4096, hidden_dim=75, num_layers=4):\n",
    "    \"\"\"\n",
    "    Train Supervised Autoencoder\n",
    "    \"\"\"\n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    dataset = LUTDataset(data_path)\n",
    "    \n",
    "    # Train/val split (90/10 from paper)\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size]\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Train samples: {train_size:,}\")\n",
    "    print(f\"Val samples: {val_size:,}\")\n",
    "    \n",
    "    # Model\n",
    "    model = SupervisedAutoencoder(\n",
    "        hidden_dim=hidden_dim, \n",
    "        num_layers=num_layers\n",
    "    ).to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = SAELoss(alpha=0.3, beta=0.3, gamma=0.6)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=10\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for rgb, params_true in train_loader:\n",
    "            rgb = rgb.to(device)\n",
    "            params_true = params_true.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            params_pred, rgb_recon = model(rgb)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss, enc_loss, dec_loss, e2e_loss = criterion(\n",
    "                rgb, params_pred, params_true, rgb_recon\n",
    "            )\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for rgb, params_true in val_loader:\n",
    "                rgb = rgb.to(device)\n",
    "                params_true = params_true.to(device)\n",
    "                \n",
    "                params_pred, rgb_recon = model(rgb)\n",
    "                loss, _, _, _ = criterion(rgb, params_pred, params_true, rgb_recon)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "              f\"Train Loss: {train_loss:.6f} | \"\n",
    "              f\"Val Loss: {val_loss:.6f} | \"\n",
    "              f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "            }, 'best_sae_model.pt')\n",
    "            print(f\"  → Saved best model (val_loss: {val_loss:.6f})\")\n",
    "    \n",
    "    print(f\"\\nTraining complete! Best val loss: {best_val_loss:.6f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Train with paper's configuration\n",
    "    model = train_sae(\n",
    "        data_path='synthetic_data.parquet',  # Use Parquet!\n",
    "        epochs=200,\n",
    "        batch_size=4096,\n",
    "        hidden_dim=75,  # Paper uses 75\n",
    "        num_layers=4    # Paper uses 4 layers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc659287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4363a649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4ac264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0fcaef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-albedo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
