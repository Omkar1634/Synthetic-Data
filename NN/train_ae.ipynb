{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_modules import *\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "import my_config\n",
    "import math\n",
    "import importlib\n",
    "\n",
    "from Landmark import gmm_specular, gmm_shadow, preprocess, plotting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    for device in physical_devices:\n",
    "        print(\"Device:\", device)\n",
    "else:\n",
    "    print(\"No GPU devices found.\")\n",
    "\n",
    "# Set GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, False)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Test for GPU device name\n",
    "name = tf.test.gpu_device_name()\n",
    "if name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(name))\n",
    "\n",
    "# Print the number of available GPUs\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load LUT data and find duplicates, select one of each set where the likeleyhood of the parameter values is the greatest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bounds = [0.99, 0.99, 0.99, 0.99, 0.25]\n",
    "lower_bounds = [0.01, 0.001, 0.01, 0.6, 0.05]\n",
    "\n",
    "# Pair up the corresponding bounds and calculate their average\n",
    "averages = [(u + l) / 2 for u, l in zip(upper_bounds, lower_bounds)]\n",
    "avg_Cm, avg_Ch, avg_Bm, avg_Bh, avg_T = averages\n",
    "\n",
    "headers = \"Cm,Ch,Bm,Bh,T,sR,sG,sB\"\n",
    "lut_path = r\"C:\\joel_cpp\\FullMonte\\FullMonte\\lut_rgb.csv\"\n",
    "\n",
    "df = pd.read_csv(lut_path, names=headers.split(\",\"), header=None)\n",
    "#remove row 0\n",
    "df = df.iloc[1:]\n",
    "print(df.head())\n",
    "\n",
    "#print length of df\n",
    "print(len(df))\n",
    "# convert all columns to float\n",
    "for column in df.columns:\n",
    "    df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "#convert sR, sG, sB to int\n",
    "rounded_df = df.round({'sR': 0, 'sG': 0, 'sB': 0})\n",
    "# Step 1: Find duplicate RGB values\n",
    "duplicates = rounded_df[rounded_df.duplicated(subset=['sR', 'sG', 'sB'], keep=False)].copy() # added .copy()\n",
    "#print all duplicates\n",
    "print(duplicates.head())\n",
    "\n",
    "# Step 2: Calculate the 'likelihood' score for each row\n",
    "duplicates['likelihood'] = (abs(duplicates['Cm'] - avg_Cm) +\n",
    "                            abs(duplicates['Ch'] - avg_Ch) +\n",
    "                            abs(duplicates['Bm'] - avg_Bm) +\n",
    "                            abs(duplicates['Bh'] - avg_Bh) +\n",
    "                            abs(duplicates['T'] - avg_T))\n",
    "\n",
    "# Step 3: Sort by RGB values and likelihood, keeping the row with the lowest likelihood for each RGB group\n",
    "most_likely_duplicates = duplicates.sort_values(['sR', 'sG', 'sB', 'likelihood']).drop_duplicates(subset=['sR', 'sG', 'sB'])\n",
    "\n",
    "# Now, most_likely_duplicates should contain your desired rows\n",
    "\n",
    "# First, remove all duplicates from the original dataframe\n",
    "df_no_duplicates = df.drop_duplicates(subset=['sR', 'sG', 'sB'], keep=False)\n",
    "\n",
    "# Concatenate df_no_duplicates with most_likely_duplicates to get the final dataframe\n",
    "df = pd.concat([df_no_duplicates, most_likely_duplicates])\n",
    "\n",
    "# If you want to sort it based on index\n",
    "df.sort_index(inplace=True)\n",
    "df.head()\n",
    "#remove duplicates\n",
    "\n",
    "x = df[['sR', 'sG', 'sB']].to_numpy(dtype='float32')\n",
    "y = df[['Cm', 'Ch', 'Bm', 'Bh', 'T']].to_numpy(dtype='float32')\n",
    "#create new csv with headers\n",
    "# df.to_csv(r'LUTs\\large_no_duplicates.csv', index=False, header=True)\n",
    "\n",
    "\n",
    "#train nn on x,y\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "#remove any header values\n",
    "x_train = x_train[1:]\n",
    "x_test = x_test[1:]\n",
    "y_train = y_train[1:]\n",
    "y_test = y_test[1:]\n",
    "\n",
    "#numpy arrays\n",
    "x_train = np.asarray(x_train).reshape(-1,3).astype('float32')\n",
    "x_test = np.asarray(x_test).reshape(-1,3).astype('float32')\n",
    "print(f\"length of df {len(df)}\")\n",
    "print(f\"bef norm x_train[0] {x_train[0]}\")\n",
    "\n",
    "#normalize\n",
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0\n",
    "print(f\"aft norm x_train[0] {x_train[0]}\")\n",
    "\n",
    "print(f\"length of x_train {len(x_train)}\")\n",
    "print(f\"length of x_test {len(x_test)}\")\n",
    "print(f\"length of y_train {len(y_train)}\")\n",
    "print(f\"length of y_test {len(y_test)}\")\n",
    "df.head()\n",
    "print(f\"length of df {len(df)}\")\n",
    "#print random 3 rows\n",
    "#print unique values of Cm,Ch,Bm,Bh,T\n",
    "# print(f\"unique Cm {df['Cm'].unique()}\")\n",
    "# print(f\"unique Ch {df['Ch'].unique()}\")\n",
    "# print(f\"unique Bm {df['Bm'].unique()}\")\n",
    "# print(f\"unique Bh {df['Bh'].unique()}\")\n",
    "# print(f\"unique T {df['T'].unique()}\")\n",
    "#as sorted lists\n",
    "C_m = sorted(df['Cm'].unique())\n",
    "C_h = sorted(df['Ch'].unique())\n",
    "B_m = sorted(df['Bm'].unique())\n",
    "B_h = sorted(df['Bh'].unique())\n",
    "T = sorted(df['T'].unique())\n",
    "print(f\"Cm = {C_m}\")\n",
    "print(f\"Ch = {C_h}\")\n",
    "print(f\"Bm = {B_m}\")\n",
    "print(f\"Bh = {B_h}\")\n",
    "print(f\"T = {T}\")\n",
    "#min max for each\n",
    "min_vals = [min(C_m), min(C_h), min(B_m), min(B_h), min(T)]\n",
    "max_vals = [max(C_m), max(C_h), max(B_m), max(B_h), max(T)]\n",
    "print(f\"upper bounds = {max_vals}\")\n",
    "print(f\"lower bounds = {min_vals}\")\n",
    "#integer arrays for sR,sG,sB 0 to 255\n",
    "# Assuming df is your DataFrame and it has columns 'sR', 'sG', 'sB'\n",
    "df[['sR', 'sG', 'sB']] = df[['sR', 'sG', 'sB']].astype(float)\n",
    "df[['sR', 'sG', 'sB']] = df[['sR', 'sG', 'sB']].applymap(np.round)\n",
    "# Add a 'count' column that counts the number of identical RGB values\n",
    "df['count'] = df.groupby(['sR', 'sG', 'sB'])['sR'].transform('count')\n",
    "print(f\"number of repeated RGB values {len(df[df['count'] > 1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "BATCH_SIZE = 4096*16\n",
    "NUM_NEURONS = 75\n",
    "NUM_LAYERS = 2\n",
    "NUM_EPOCHS = 200\n",
    "LR = 1e-4\n",
    "MLR = 1e-6\n",
    "about_string = f\"batch_size_{BATCH_SIZE}_neurons_{NUM_NEURONS}_layers_{NUM_LAYERS}_epochs_{NUM_EPOCHS}_lr_{LR}_mlr_{MLR}_upper_bounds_{upper_bounds}_lower_bounds_{lower_bounds}\"\n",
    "\n",
    "def decoder():\n",
    "    input = Input(shape=(5,), name=\"decoder_input\")\n",
    "    for i in range(NUM_LAYERS):\n",
    "        if i == 0:\n",
    "            x = Dense(NUM_NEURONS, activation='relu', name=f\"decoder_dense_{i+1}\")(input)\n",
    "        else:\n",
    "            x = Dense(NUM_NEURONS, activation='relu', name=f\"decoder_dense_{i+1}\")(x)\n",
    "    out = Dense(3, name=\"encoder_output\")(x)\n",
    "    #leaky relu activation=LeakyReLU(alpha=0.3)\n",
    "    model = Model(inputs=input, outputs=out, name='decoder')\n",
    "    return model\n",
    "\n",
    "def encoder():\n",
    "    input = Input(shape=(3,),name=\"encoder_input\")\n",
    "    for i in range(NUM_LAYERS):\n",
    "        if i == 0:\n",
    "            x = Dense(NUM_NEURONS, activation='relu', name=f\"encoder_dense_{i+1}\")(input)\n",
    "        else:\n",
    "            x = Dense(NUM_NEURONS, activation='relu', name=f\"encoder_dense_{i+1}\")(x)\n",
    "    out = Dense(5, name=\"decoder_output\")(x)\n",
    "    model = Model(inputs=input, outputs=out, name = 'encoder')\n",
    "    return model\n",
    "def autoencoder(encoder, decoder):\n",
    "    input_end_to_end = Input(shape=(3,))\n",
    "    l1 = encoder(input_end_to_end)\n",
    "    l2 = decoder(l1)\n",
    "    input_list = [encoder.input, decoder.input, input_end_to_end]\n",
    "    output_list = [encoder.output, decoder.output, l2]\n",
    "    model = Model(inputs=input_list, outputs=output_list, name = 'autoencoder')\n",
    "    return model\n",
    "encoder = encoder()\n",
    "decoder = decoder()\n",
    "autoencoder = autoencoder(encoder, decoder)\n",
    "print(encoder.summary())\n",
    "print(decoder.summary())\n",
    "print(autoencoder.summary())\n",
    "\n",
    "def albedo_loss(y_true, y_pred):\n",
    "    #l1 norm\n",
    "    l1_norm = K.sum(K.abs(y_pred - y_true), axis=-1)\n",
    "    l2_norm = K.sqrt(K.sum(K.square(y_pred - y_true), axis=-1))\n",
    "    # upper_bounds = [1.0, 1.0, 1.0]\n",
    "    # lower_bounds = [0.0, 0.0, 0.0]\n",
    "    # upper_bound_mask = tf.math.greater(y_pred, upper_bounds)\n",
    "    # lower_bound_mask = tf.math.less(y_pred, lower_bounds)\n",
    "    # out_of_range_mask = tf.logical_or(lower_bound_mask, upper_bound_mask)\n",
    "    # # Apply a penalty for out of range values)\n",
    "    # out_of_range_penalty = tf.reduce_sum(tf.cast(out_of_range_mask, tf.float32))/BATCH_SIZE\n",
    "    return  l1_norm\n",
    "\n",
    "def parameter_loss(y_true, y_pred):\n",
    "    #l2 norm\n",
    "    l2_norm = K.sqrt(K.sum(K.square(y_pred - y_true), axis=-1))\n",
    "    return  l2_norm \n",
    "    \n",
    "def end_to_end_loss(y_true, y_pred):\n",
    "    #l1 norm\n",
    "    l1_norm = K.sum(K.abs(y_pred - y_true), axis=-1)\n",
    "    # l2_norm = K.sqrt(K.sum(K.square(y_pred - y_true), axis=-1))\n",
    "    return l1_norm \n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss=[parameter_loss, albedo_loss, end_to_end_loss], loss_weights=[.3,.1,.6])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(my_config.CHECKPOINT_PATH, monitor='loss', verbose=0,\n",
    "    save_best_only=True, mode='auto', period=200)\n",
    "adjust_lr = ReduceLROnPlateau(monitor='loss', factor=0.01, patience=5, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=MLR, lr=LR)\n",
    "# Define the Keras TensorBoard callback.\n",
    "\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(filepath=logdir, save_weights_only = True,save_freq = 100,verbose = 1)\n",
    "print_callback = LambdaCallback(on_epoch_end=lambda epoch, logs: print(f\"epoch: {epoch} , {logs}\") if epoch % 25 == 0 else None)\n",
    "\n",
    "callbacks = [\n",
    "    checkpoint,\n",
    "    adjust_lr,\n",
    "    print_callback\n",
    "]\n",
    "with tf.device('/device:GPU:0') as device:\n",
    "    #show device name\n",
    "    print(device)\n",
    "    #ae_in: enc_in, dec_in, end_to_end_in\n",
    "    x = [x_train, y_train,x_train]\n",
    "     #ae_out: enc_out, dec_out, end_to_end_out\n",
    "    x_val = [x_test, y_test,x_test]\n",
    "    #outputs: encoder, decoder, autoencoder\n",
    "    y = [y_train,x_train,x_train]\n",
    "    y_val = [y_test,x_test,x_test]\n",
    "    autoencoder.fit(x,y, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, shuffle=True, validation_data=(x_val, y_val), callbacks=callbacks,verbose=0)\n",
    "\n",
    "       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')\n",
    "plt.rcParams['axes.grid'] = False\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "plt.rcParams['ytick.labelsize'] = 8\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "plt.rcParams['ytick.labelsize'] = 8\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.plot(autoencoder.history.history['loss']) \n",
    "plt.plot(autoencoder.history.history['val_loss'])\n",
    "plt.title('model loss \\n = encoder loss (L1 norm) + \\n decoder loss (L2 norm) +\\n end to end loss (L1 norm) ')\n",
    "plt.ylabel('loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('epoch')\n",
    "plt.yticks\n",
    "# plt.xlim(-1,50)\n",
    "plt.tight_layout()\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "#show tick labels\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "plt.rcParams['ytick.labelsize'] = 8\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(image):\n",
    "    print(f\"Image shape at start of encoder method: {image.shape}\")\n",
    "    if len(image.shape) == 2:\n",
    "        WIDTH = HEIGHT = int(math.sqrt(image.shape[0]))\n",
    "        image = np.asarray(image).reshape(-1,4).astype('float32')\n",
    "    else:\n",
    "        WIDTH = image.shape[0]\n",
    "        HEIGHT = image.shape[1]\n",
    "        image = np.asarray(image).astype('float32')\n",
    "    image = image.reshape(WIDTH*HEIGHT, 3) \n",
    "    start = time.time()\n",
    "    with tf.device('/device:GPU:0') as device:\n",
    "        print(f\"Image shape before encoder inference: {image.shape}\")\n",
    "        pred_maps = encoder.predict_on_batch(image)\n",
    "    #reshape to 1\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    #reshape to -1,3\n",
    "    pred_maps = pred_maps.reshape(WIDTH*HEIGHT, 5)  # for RGB image\n",
    "    return pred_maps, elapsed, (WIDTH, HEIGHT)\n",
    " \n",
    "def decode(encoded):\n",
    "    print(f\"Image shape going into encoder: {encoded.shape}\")\n",
    "    start = time.time()\n",
    "    #check if encoded is 2d or 3d\n",
    "    if len(encoded.shape) == 2:\n",
    "        WIDTH = HEIGHT = int(math.sqrt(encoded.shape[0]))\n",
    "        encoded = np.asarray(encoded).reshape(-1,5).astype('float32')\n",
    "    else:\n",
    "        WIDTH = encoded.shape[0]\n",
    "        HEIGHT = encoded.shape[1]\n",
    "        encoded = np.asarray(encoded).astype('float32')\n",
    "    with tf.device('/device:GPU:0') as device:\n",
    "        #lower batch size to 2048\n",
    "        print(f\"encoded shape going into decoder: {encoded.shape}\")\n",
    "        recovered = decoder.predict_on_batch(encoded)\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    recovered = recovered.reshape(WIDTH, HEIGHT, 3)  # for RGB image\n",
    "    return recovered, elapsed, (WIDTH, HEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 256\n",
    "HEIGHT = 256\n",
    "importlib.reload(gmm_specular)\n",
    "importlib.reload(gmm_shadow)\n",
    "importlib.reload(preprocess)\n",
    "importlib.reload(plotting)\n",
    "\n",
    "#dark background style\n",
    "plt.style.use('dark_background')\n",
    "#grid off \n",
    "plt.rcParams['axes.grid'] = False\n",
    "#reload\n",
    "importlib.reload(gmm_specular)\n",
    "importlib.reload(gmm_shadow)\n",
    "# Load your image\n",
    "image_rgb = cv2.imread( r\"C:\\Users\\joeli\\Dropbox\\HM_Oct\\models_4k\\m32.png\")\n",
    "image_rgb = cv2.cvtColor(image_rgb, cv2.COLOR_BGR2RGB)\n",
    "image_rgb = preprocess.crop_face(image_rgb)[0]\n",
    "image_rgb = cv2.resize(image_rgb, (WIDTH, HEIGHT))\n",
    "#numpy array\n",
    "image_rgb = np.asarray(image_rgb).astype('float32')/255.0\n",
    "plt.imshow(image_rgb)\n",
    "plt.title(\"original\")\n",
    "plt.show()\n",
    "parameter_maps, elapsed, d1 = encode(image_rgb)\n",
    "pm1 = parameter_maps.copy().reshape(d1[0], d1[1], 5)\n",
    "#normalize\n",
    "Cm = parameter_maps[:,0]\n",
    "Ch = parameter_maps[:,1]\n",
    "Bm = parameter_maps[:,2]\n",
    "Bh = parameter_maps[:,3]\n",
    "T = parameter_maps[:,4]\n",
    "#normalize\n",
    "Cm = (Cm - np.min(Cm))/(np.max(Cm) - np.min(Cm))\n",
    "Ch = (Ch - np.min(Ch))/(np.max(Ch) - np.min(Ch))\n",
    "Bm = (Bm - np.min(Bm))/(np.max(Bm) - np.min(Bm))\n",
    "Bh = (Bh - np.min(Bh))/(np.max(Bh) - np.min(Bh))\n",
    "T = (T - np.min(T))/(np.max(T) - np.min(T))\n",
    "parameter_maps[:,0] = Cm*0.62+0.001\n",
    "parameter_maps[:,1] = Ch*0.31+0.001\n",
    "parameter_maps[:,2] = Bm*0.8+0.2\n",
    "parameter_maps[:,3] = Bh*0.3+0.6\n",
    "parameter_maps[:,4] = T*0.2+0.05\n",
    "print(f\"CM {Cm.min()} {Cm.max()}\")\n",
    "print(f\"CH {Ch.min()} {Ch.max()}\")\n",
    "print(f\"BM {Bm.min()} {Bm.max()}\")\n",
    "print(f\"BH {Bh.min()} {Bh.max()}\")\n",
    "print(f\"T {T.min()} {T.max()}\")\n",
    "WIDTH, HEIGHT = d1\n",
    "print(f\"encode time {elapsed}\")\n",
    "\n",
    "recovered,elapsed, d2 = decode(pm1.reshape(-1,5))\n",
    "# recovered = cv2.cvtColor(recovered, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "WIDTH, HEIGHT = d2\n",
    "recovered = recovered\n",
    "\n",
    "plotting.PLOT_TEX_MAPS(recovered, parameter_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-albedo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
